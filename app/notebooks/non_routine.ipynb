{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Not working atm, accidently broke"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Non-derivative transaction data:\n",
      "| ACCESSION_NUMBER     | TRANS_DATE   | TRANS_CODE   |   EQUITY_SWAP_INVOLVED |   TRANS_SHARES |   TRANS_PRICEPERSHARE | TRANS_ACQUIRED_DISP_CD   |   SHRS_OWND_FOLWNG_TRANS | DIRECT_INDIRECT_OWNERSHIP   |   COMPANY_ID | RPTOWNERNAME     | RPTOWNER_RELATIONSHIP   | RPTOWNER_TITLE         |\n",
      "|:---------------------|:-------------|:-------------|-----------------------:|---------------:|----------------------:|:-------------------------|-------------------------:|:----------------------------|-------------:|:-----------------|:------------------------|:-----------------------|\n",
      "| 0000076605-17-000121 | 2017-09-27   | S            |                      0 |          10000 |                 83.16 | D                        |          15000           | D                           |       295249 | Cleveland Todd M | Director,Officer        | CEO                    |\n",
      "| 0001140361-17-037031 | 2017-09-27   | F            |                      0 |          28833 |                 16.46 | D                        |              3.36052e+06 | D                           |     54070696 | Olson Michael    | Officer                 | Chief Strategy Officer |\n",
      "| 0001140361-17-037030 | 2017-04-27   | M            |                      0 |          11875 |                  0    | A                        |              3.36052e+06 | D                           |     54070696 | Olson Michael    | Officer                 | Chief Strategy Officer |\n",
      "| 0001140361-17-037030 | 2017-09-15   | M            |                      0 |           9063 |                  0    | A                        |              3.36052e+06 | D                           |     54070696 | Olson Michael    | Officer                 | Chief Strategy Officer |\n",
      "| 0001140361-17-037030 | 2017-09-15   | M            |                      0 |           8333 |                  0    | A                        |              3.36052e+06 | D                           |     54070696 | Olson Michael    | Officer                 | Chief Strategy Officer |\n",
      "\n",
      "Returns and market cap data:\n",
      "| MONTH_END   |   COMPANY_ID |   RETURN_LEAD_1_MONTHS |   RETURN_LEAD_3_MONTHS |   RETURN_LEAD_12_MONTHS |   MARKET_CAP_USD |\n",
      "|:------------|-------------:|-----------------------:|-----------------------:|------------------------:|-----------------:|\n",
      "| 1994-07-31  |        18511 |             0.106531   |              0.109966  |                0.375126 |          2616.76 |\n",
      "| 1994-08-31  |        18511 |             0.0139749  |              0.043478  |                0.255416 |          2885.57 |\n",
      "| 1994-09-30  |        18511 |            -0.0107207  |              0.0316986 |                0.261565 |          3001.81 |\n",
      "| 1994-10-31  |        18511 |             0.0402486  |              0.083469  |                0.292618 |          3078.85 |\n",
      "| 1994-11-30  |        18511 |             0.00252853 |              0.0355451 |                0.279064 |          3069.56 |\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pandas.tseries.offsets import MonthEnd\n",
    "import time\n",
    "\n",
    "DATA_DIR = \"../data\"\n",
    "\n",
    "# returns_and_market_cap contains monthly returns and market cap data per company\n",
    "returns_and_market_cap = pd.read_csv(f\"{DATA_DIR}/returns_market_cap.csv\")\n",
    "\n",
    "# ndt contains non-derivative transaction data (e.g., insider trades)\n",
    "ndt = pd.read_csv(f\"{DATA_DIR}/ndt.csv\")\n",
    "\n",
    "print(\"Non-derivative transaction data:\")\n",
    "print(ndt.head().to_markdown(index=False))\n",
    "print()\n",
    "print(\"Returns and market cap data:\")\n",
    "print(returns_and_market_cap.head().to_markdown(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1: Data Preparation and Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Dropped 15 rows with invalid TRANS_DATE.\n",
      "Sample of trades data after preparation:\n",
      "| ACCESSION_NUMBER     | TRANS_DATE          | TRANS_CODE   |   EQUITY_SWAP_INVOLVED |   TRANS_SHARES |   TRANS_PRICEPERSHARE | TRANS_ACQUIRED_DISP_CD   |   SHRS_OWND_FOLWNG_TRANS | DIRECT_INDIRECT_OWNERSHIP   |   COMPANY_ID | RPTOWNERNAME   | RPTOWNER_RELATIONSHIP   | RPTOWNER_TITLE                 | month_end           |      trade_value |\n",
      "|:---------------------|:--------------------|:-------------|-----------------------:|---------------:|----------------------:|:-------------------------|-------------------------:|:----------------------------|-------------:|:---------------|:------------------------|:-------------------------------|:--------------------|-----------------:|\n",
      "| 0000001750-06-000002 | 2006-01-04 00:00:00 | S            |                      0 |         144360 |                 24.64 | D                        |                  6876.17 | D                           |       168154 | STORCH DAVID P | Director,Officer        | Chairman, Pres., CEO, Director | 2006-01-31 00:00:00 |      3.55703e+06 |\n",
      "| 0000001750-06-000054 | 2006-03-28 00:00:00 | S            |                      0 |          66200 |                 27.46 | D                        |                 18810    | D                           |       168154 | STORCH DAVID P | Director,Officer        | President, CEO & Director      | 2006-03-31 00:00:00 |      1.81785e+06 |\n",
      "| 0000001750-06-000056 | 2006-03-29 00:00:00 | S            |                      0 |         290912 |                 27.47 | D                        |                  6876.17 | D                           |       168154 | STORCH DAVID P | Director,Officer        | President, CEO & Director      | 2006-03-31 00:00:00 |      7.99135e+06 |\n",
      "| 0000001750-06-000060 | 2006-03-31 00:00:00 | S            |                      0 |          50000 |                 28.4  | D                        |                 18810    | D                           |       168154 | STORCH DAVID P | Director,Officer        | President, CEO & Director      | 2006-03-31 00:00:00 |      1.42e+06    |\n",
      "| 0000001750-06-000062 | 2006-04-03 00:00:00 | S            |                      0 |          24010 |                 29.14 | D                        |                 18810    | D                           |       168154 | STORCH DAVID P | Director,Officer        | President, CEO & Director      | 2006-04-30 00:00:00 | 699651           |\n",
      "\n",
      "Shape of trades data after preparation: (326634, 15)\n"
     ]
    }
   ],
   "source": [
    "# Convert date columns to datetime for consistent handling\n",
    "# MONTH_END in returns_and_market_cap is the end-of-month date for returns/market cap\n",
    "returns_and_market_cap['MONTH_END'] = pd.to_datetime(returns_and_market_cap['MONTH_END'])\n",
    "\n",
    "# TRANS_DATE in ndt is the transaction date; use errors='coerce' to handle invalid dates\n",
    "ndt['TRANS_DATE'] = pd.to_datetime(ndt['TRANS_DATE'], errors='coerce')\n",
    "\n",
    "# Create a month-end column in ndt by rounding transaction dates to the nearest month end\n",
    "ndt['month_end'] = ndt['TRANS_DATE'] + MonthEnd(0)\n",
    "\n",
    "# Check for and drop rows with invalid transaction dates (NaT)\n",
    "invalid_dates = ndt['TRANS_DATE'].isna().sum()\n",
    "if invalid_dates > 0:\n",
    "    print(f\"Warning: Dropped {invalid_dates} rows with invalid TRANS_DATE.\")\n",
    "    ndt = ndt.dropna(subset=['TRANS_DATE'])\n",
    "\n",
    "# Filter ndt to include only buy ('P') and sell ('S') transactions\n",
    "trades = ndt[ndt['TRANS_CODE'].isin(['P', 'S'])].copy()\n",
    "\n",
    "# Collapse duplicate rows so each ACCESSION_NUMBER is one trade\n",
    "trades = trades.groupby('ACCESSION_NUMBER', as_index=False).agg({\n",
    "    'TRANS_DATE': 'first',\n",
    "    'TRANS_CODE': 'first',\n",
    "    'EQUITY_SWAP_INVOLVED': 'first',\n",
    "    'TRANS_SHARES': 'sum',\n",
    "    'TRANS_PRICEPERSHARE': 'first',\n",
    "    'TRANS_ACQUIRED_DISP_CD': 'first',\n",
    "    'SHRS_OWND_FOLWNG_TRANS': 'first',\n",
    "    'DIRECT_INDIRECT_OWNERSHIP': 'first',\n",
    "    'COMPANY_ID': 'first',\n",
    "    'RPTOWNERNAME': 'first',\n",
    "    'RPTOWNER_RELATIONSHIP': 'first',\n",
    "    'RPTOWNER_TITLE': 'first',\n",
    "    'month_end': 'first'\n",
    "})\n",
    "\n",
    "\n",
    "# Calculate the dollar value of each trade (shares * price per share)\n",
    "trades['trade_value'] = trades['TRANS_SHARES'] * trades['TRANS_PRICEPERSHARE']\n",
    "\n",
    "print(\"Sample of trades data after preparation:\")\n",
    "print(trades.head().to_markdown(index=False))\n",
    "print()\n",
    "print(\"Shape of trades data after preparation:\", trades.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2: Market Returns Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample of market_return_1m market returns:\n",
      "| MONTH_END           |   market_return_1m |\n",
      "|:--------------------|-------------------:|\n",
      "| 1962-01-31 00:00:00 |          0.0886072 |\n",
      "| 1962-02-28 00:00:00 |         -0.0697672 |\n",
      "| 1962-03-31 00:00:00 |          0.0249994 |\n",
      "| 1962-04-30 00:00:00 |         -0.414634  |\n",
      "| 1962-05-31 00:00:00 |          0.0520832 |\n",
      "Sample of market_return_3m market returns:\n",
      "| MONTH_END           |   market_return_3m |\n",
      "|:--------------------|-------------------:|\n",
      "| 1962-01-31 00:00:00 |           0.037974 |\n",
      "| 1962-02-28 00:00:00 |          -0.44186  |\n",
      "| 1962-03-31 00:00:00 |          -0.36875  |\n",
      "| 1962-04-30 00:00:00 |          -0.256097 |\n",
      "| 1962-05-31 00:00:00 |           0.375    |\n",
      "Sample of market_return_12m market returns:\n",
      "| MONTH_END           |   market_return_12m |\n",
      "|:--------------------|--------------------:|\n",
      "| 1962-01-31 00:00:00 |           -0.319621 |\n",
      "| 1962-02-28 00:00:00 |           -0.331395 |\n",
      "| 1962-03-31 00:00:00 |           -0.234376 |\n",
      "| 1962-04-30 00:00:00 |           -0.14939  |\n",
      "| 1962-05-31 00:00:00 |            0.510417 |\n"
     ]
    }
   ],
   "source": [
    "# Compute total market capitalization per month for weighting returns\n",
    "# Sum MARKET_CAP_USD across all companies for each MONTH_END\n",
    "total_cap = returns_and_market_cap.groupby('MONTH_END')['MARKET_CAP_USD'].sum().rename('total_cap')\n",
    "\n",
    "# Join total_cap back to returns_and_market_cap for weight calculation\n",
    "returns_and_market_cap = returns_and_market_cap.join(total_cap, on='MONTH_END')\n",
    "\n",
    "# Calculate each company's weight as its market cap divided by total market cap\n",
    "returns_and_market_cap['weight'] = returns_and_market_cap['MARKET_CAP_USD'] / returns_and_market_cap['total_cap']\n",
    "\n",
    "# Initialize a dictionary to store market returns for different horizons (1, 3, 12 months)\n",
    "market_returns = {}\n",
    "\n",
    "# Compute weighted market returns for each horizon\n",
    "# For each month, multiply individual company returns by their weights and sum\n",
    "for horizon in ['1', '3', '12']:\n",
    "    market_returns[f'market_return_{horizon}m'] = returns_and_market_cap.groupby('MONTH_END').apply(\n",
    "        lambda df: np.sum(df[f'RETURN_LEAD_{horizon}_MONTHS'] * df['weight']),\n",
    "        include_groups=False  # Avoid including group keys in the apply function\n",
    "    ).reset_index(name=f'market_return_{horizon}m')\n",
    "\n",
    "for horizon in ['1', '3', '12']:\n",
    "    print(f\"Sample of market_return_{horizon}m market returns:\")\n",
    "    print(market_returns[f'market_return_{horizon}m'].head().to_markdown(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3: Merging Trades with Returns Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample of merged data with excess returns:\n",
      "|   COMPANY_ID | month_end           | TRANS_CODE   |   excess_return_1m |   excess_return_3m |   excess_return_12m |\n",
      "|-------------:|:--------------------|:-------------|-------------------:|-------------------:|--------------------:|\n",
      "|       168154 | 2006-01-31 00:00:00 | S            |          0.0584863 |          0.0745878 |           0.0870041 |\n",
      "|       168154 | 2006-03-31 00:00:00 | S            |         -0.0752978 |         -0.193422  |          -0.165043  |\n",
      "|       168154 | 2006-03-31 00:00:00 | S            |         -0.0752978 |         -0.193422  |          -0.165043  |\n",
      "|       168154 | 2006-03-31 00:00:00 | S            |         -0.0752978 |         -0.193422  |          -0.165043  |\n",
      "|       168154 | 2006-04-30 00:00:00 | S            |         -0.0544755 |         -0.0779137 |          -0.0112499 |\n"
     ]
    }
   ],
   "source": [
    "# Merge trades with returns_and_market_cap to align trade data with returns and market cap\n",
    "# Use left join to keep all trades, even if no matching returns data exists\n",
    "merged = trades.merge(returns_and_market_cap, \n",
    "                      left_on=['COMPANY_ID', 'month_end'], \n",
    "                      right_on=['COMPANY_ID', 'MONTH_END'], \n",
    "                      how='left').drop(columns='MONTH_END')  # Drop redundant MONTH_END column\n",
    "\n",
    "# Merge in the precomputed market returns for each horizon\n",
    "for horizon, df in market_returns.items():\n",
    "    merged = merged.merge(df, \n",
    "                          left_on='month_end', \n",
    "                          right_on='MONTH_END', \n",
    "                          how='left').drop(columns='MONTH_END')\n",
    "\n",
    "# Calculate excess returns for each horizon (company return - market return)\n",
    "for horizon in ['1', '3', '12']:\n",
    "    merged[f'excess_return_{horizon}m'] = merged[f'RETURN_LEAD_{horizon}_MONTHS'] - merged[f'market_return_{horizon}m']\n",
    "\n",
    "print(\"Sample of merged data with excess returns:\")\n",
    "print(merged[['COMPANY_ID', 'month_end', 'TRANS_CODE', 'excess_return_1m', 'excess_return_3m', 'excess_return_12m']].head().to_markdown(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4: Non-routine and Quantile Assignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/d2/h05xc7x1181fd8jxpltdv9v40000gn/T/ipykernel_46910/2629139255.py:87: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  monthly = monthly.groupby('COMPANY_ID').apply(compute_large_txn_score).reset_index(drop=True)\n",
      "/var/folders/d2/h05xc7x1181fd8jxpltdv9v40000gn/T/ipykernel_46910/2629139255.py:115: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  monthly = monthly.groupby('COMPANY_ID').apply(compute_pattern_score).reset_index(drop=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nonroutine insider quartile counts:\n",
      "nonroutine_insider_quartile\n",
      "0    30636\n",
      "1    30639\n",
      "2    30631\n",
      "3    30635\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Sample of monthly scores:\n",
      "   COMPANY_ID  month_end  pct_score  large_txn_score  pattern_score  \\\n",
      "0       18671 2006-01-31   0.968910         0.000000            0.0   \n",
      "1       18671 2006-02-28   0.761748         0.000000            0.0   \n",
      "2       18671 2006-03-31   0.000000         0.000000            0.0   \n",
      "3       18671 2006-04-30   0.865940         0.000000            0.0   \n",
      "4       18671 2006-06-30   0.996437         0.000000            0.0   \n",
      "5       18671 2006-07-31   0.917653         0.267124            0.0   \n",
      "6       18671 2006-10-31   0.691644         0.000000            0.0   \n",
      "7       18671 2006-12-31   0.308334         0.666702            0.0   \n",
      "8       18671 2007-02-28   0.992893         0.086947            0.0   \n",
      "9       18671 2007-03-31   0.961874         0.000000            0.0   \n",
      "\n",
      "   cluster_score  signal_score  nonroutine_insider_quartile  \n",
      "0            0.0      0.290673                            2  \n",
      "1            0.0      0.228524                            1  \n",
      "2            0.0      0.000000                            0  \n",
      "3            0.0      0.259782                            2  \n",
      "4            0.0      0.298931                            2  \n",
      "5            0.0      0.342077                            3  \n",
      "6            0.0      0.207493                            1  \n",
      "7            0.0      0.259176                            2  \n",
      "8            1.0      0.519605                            3  \n",
      "9            0.0      0.288562                            2  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/d2/h05xc7x1181fd8jxpltdv9v40000gn/T/ipykernel_46910/2629139255.py:137: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  monthly = monthly.groupby('COMPANY_ID').apply(compute_cluster_score).reset_index(drop=True)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# ====================================================\n",
    "# STEP 1: Compute per‐trade metrics\n",
    "# ====================================================\n",
    "merged['original_shares'] = np.where(\n",
    "    merged['TRANS_CODE'] == 'S',\n",
    "    merged['SHRS_OWND_FOLWNG_TRANS'] + merged['TRANS_SHARES'],\n",
    "    np.where(merged['TRANS_CODE'] == 'P',\n",
    "             merged['SHRS_OWND_FOLWNG_TRANS'] - merged['TRANS_SHARES'],\n",
    "             np.nan)\n",
    ")\n",
    "merged = merged.query(\"original_shares > 0\").copy()\n",
    "\n",
    "merged['pct_holdings_sold'] = np.where(\n",
    "    merged['TRANS_CODE'] == 'S',\n",
    "    merged['TRANS_SHARES'] / merged['original_shares'],\n",
    "    np.nan\n",
    ")\n",
    "merged['pct_holdings_bought'] = np.where(\n",
    "    merged['TRANS_CODE'] == 'P',\n",
    "    merged['TRANS_SHARES'] / merged['original_shares'],\n",
    "    np.nan\n",
    ")\n",
    "merged['trade_value'] = merged['TRANS_SHARES'] * merged['TRANS_PRICEPERSHARE']\n",
    "\n",
    "# ====================================================\n",
    "# STEP 2: Aggregate to monthly level for each company\n",
    "# ====================================================\n",
    "# For percent traded, use the maximum percentage in the month.\n",
    "monthly_pct_sell = merged[merged['TRANS_CODE'] == 'S'].groupby(['COMPANY_ID','month_end'])['pct_holdings_sold'].max().reset_index(name='max_pct_sold')\n",
    "monthly_pct_buy  = merged[merged['TRANS_CODE'] == 'P'].groupby(['COMPANY_ID','month_end'])['pct_holdings_bought'].max().reset_index(name='max_pct_bought')\n",
    "\n",
    "# Aggregate other metrics:\n",
    "monthly_agg = merged.groupby(['COMPANY_ID','month_end']).agg(\n",
    "    total_trade_value = ('trade_value','sum'),\n",
    "    trade_count = ('TRANS_CODE','count'),\n",
    "    buys = ('TRANS_CODE', lambda x: (x=='P').sum()),\n",
    "    sells = ('TRANS_CODE', lambda x: (x=='S').sum())\n",
    ").reset_index()\n",
    "\n",
    "# Merge percent metrics into monthly data:\n",
    "monthly = monthly_agg.merge(monthly_pct_sell, on=['COMPANY_ID','month_end'], how='left')\n",
    "monthly = monthly.merge(monthly_pct_buy, on=['COMPANY_ID','month_end'], how='left')\n",
    "\n",
    "# ====================================================\n",
    "# STEP 3: Compute individual signal scores (historically within each company)\n",
    "# ====================================================\n",
    "\n",
    "# 3a. Percent Traded Score:\n",
    "# For sells: if max_pct_sold > 30%, score = (max_pct_sold - 0.30)/(1 - 0.30);\n",
    "# for buys: if max_pct_buy > 10%, score = (max_pct_buy - 0.10)/(1 - 0.10).\n",
    "def compute_pct_score(row):\n",
    "    score_sell = 0\n",
    "    score_buy = 0\n",
    "    if pd.notnull(row['max_pct_sold']) and row['max_pct_sold'] > 0.30:\n",
    "        score_sell = (row['max_pct_sold'] - 0.30) / (1 - 0.30)\n",
    "    if pd.notnull(row['max_pct_bought']) and row['max_pct_bought'] > 0.10:\n",
    "        score_buy = (row['max_pct_bought'] - 0.10) / (1 - 0.10)\n",
    "    return max(score_sell, score_buy)\n",
    "\n",
    "monthly['pct_score'] = monthly.apply(compute_pct_score, axis=1)\n",
    "\n",
    "# 3b. Large Transaction Volume Score:\n",
    "# For each month, compare total_trade_value to the historical median (using only prior months).\n",
    "def compute_large_txn_score(df):\n",
    "    df = df.sort_values('month_end').copy()\n",
    "    scores = []\n",
    "    for i, row in df.iterrows():\n",
    "        hist = df[df['month_end'] < row['month_end']]\n",
    "        if hist.empty or hist['total_trade_value'].median() == 0:\n",
    "            scores.append(0)\n",
    "        else:\n",
    "            med = hist['total_trade_value'].median()\n",
    "            ratio = row['total_trade_value'] / med\n",
    "            # Normalize: if ratio == 1 then score = 0; if ratio reaches the historical max, score = 1.\n",
    "            max_ratio = hist['total_trade_value'].max() / med\n",
    "            if max_ratio <= 1:\n",
    "                scores.append(0)\n",
    "            else:\n",
    "                s = (ratio - 1) / (max_ratio - 1)\n",
    "                scores.append(np.clip(s, 0, 1))\n",
    "    df['large_txn_score'] = scores\n",
    "    return df\n",
    "\n",
    "monthly = monthly.groupby('COMPANY_ID').apply(compute_large_txn_score).reset_index(drop=True)\n",
    "\n",
    "# 3c. Pattern Reversal Score:\n",
    "# For each month, compute current net direction: (buys - sells) / total trades.\n",
    "# Then, using only prior months, compute historical net.\n",
    "# If the current net is opposite in sign to historical net, assign the current net as the reversal score.\n",
    "def compute_pattern_score(df):\n",
    "    df = df.sort_values('month_end').copy()\n",
    "    scores = []\n",
    "    for i, row in df.iterrows():\n",
    "        current_total = row['buys'] + row['sells']\n",
    "        current_net = 0 if current_total == 0 else (row['buys'] - row['sells']) / current_total\n",
    "        hist = df[df['month_end'] < row['month_end']]\n",
    "        if hist.empty:\n",
    "            scores.append(0)\n",
    "        else:\n",
    "            total_hist_buys = hist['buys'].sum()\n",
    "            total_hist_sells = hist['sells'].sum()\n",
    "            hist_total = total_hist_buys + total_hist_sells\n",
    "            hist_net = 0 if hist_total == 0 else (total_hist_buys - total_hist_sells) / hist_total\n",
    "            # If current net is opposite in sign to historical net, return current_net (preserving sign)\n",
    "            if (hist_net > 0 and current_net < 0) or (hist_net < 0 and current_net > 0):\n",
    "                scores.append(current_net)\n",
    "            else:\n",
    "                scores.append(0)\n",
    "    df['pattern_score'] = scores\n",
    "    return df\n",
    "\n",
    "monthly = monthly.groupby('COMPANY_ID').apply(compute_pattern_score).reset_index(drop=True)\n",
    "\n",
    "# 3d. Cluster Trading Score:\n",
    "# For each month, compare trade_count to the historical monthly counts.\n",
    "def compute_cluster_score(df):\n",
    "    df = df.sort_values('month_end').copy()\n",
    "    scores = []\n",
    "    for i, row in df.iterrows():\n",
    "        hist = df[df['month_end'] < row['month_end']]\n",
    "        if hist.empty:\n",
    "            scores.append(0)\n",
    "        else:\n",
    "            med = hist['trade_count'].median()\n",
    "            max_count = hist['trade_count'].max()\n",
    "            if max_count == med:\n",
    "                scores.append(0)\n",
    "            else:\n",
    "                s = (row['trade_count'] - med) / (max_count - med)\n",
    "                scores.append(np.clip(s, 0, 1))\n",
    "    df['cluster_score'] = scores\n",
    "    return df\n",
    "\n",
    "monthly = monthly.groupby('COMPANY_ID').apply(compute_cluster_score).reset_index(drop=True)\n",
    "\n",
    "# ====================================================\n",
    "# STEP 4: Combine the scores using weights and compute overall signal\n",
    "# ====================================================\n",
    "weights = {\n",
    "    'pct_score': 0.25,\n",
    "    'large_txn_score': 0.25,\n",
    "    'pattern_score': 0.25,\n",
    "    'cluster_score': 0.25\n",
    "}\n",
    "\n",
    "monthly['signal_score'] = (\n",
    "    monthly['pct_score'] * weights['pct_score'] +\n",
    "    monthly['large_txn_score'] * weights['large_txn_score'] +\n",
    "    monthly['pattern_score'] * weights['pattern_score'] +\n",
    "    monthly['cluster_score'] * weights['cluster_score']\n",
    ")\n",
    "\n",
    "# ====================================================\n",
    "# STEP 5: Bin overall signal score into quartiles\n",
    "# ====================================================\n",
    "monthly['nonroutine_insider_quartile'] = pd.qcut(\n",
    "    monthly['signal_score'],\n",
    "    4,\n",
    "    labels=False,\n",
    "    duplicates='drop'\n",
    ")\n",
    "\n",
    "# ====================================================\n",
    "# Optional: Review the final monthly metrics\n",
    "# ====================================================\n",
    "print(\"Nonroutine insider quartile counts:\")\n",
    "print(monthly['nonroutine_insider_quartile'].value_counts().sort_index())\n",
    "\n",
    "print(\"\\nSample of monthly scores:\")\n",
    "print(monthly[['COMPANY_ID','month_end','pct_score','large_txn_score','pattern_score','cluster_score','signal_score','nonroutine_insider_quartile']].head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|   COMPANY_ID | month_end           |   total_trade_value |   trade_count |   buys |   sells |   max_pct_sold |   max_pct_bought |   pct_score |   large_txn_score |   pattern_score |   cluster_score |   signal_score |   nonroutine_insider_quartile |\n",
      "|-------------:|:--------------------|--------------------:|--------------:|-------:|--------:|---------------:|-----------------:|------------:|------------------:|----------------:|----------------:|---------------:|------------------------------:|\n",
      "|        18671 | 2006-01-31 00:00:00 |         5.54571e+07 |             3 |      0 |       3 |       0.978237 |              nan |    0.96891  |                 0 |               0 |               0 |       0.290673 |                             2 |\n",
      "|        18671 | 2006-02-28 00:00:00 |         7.62934e+07 |             2 |      0 |       2 |       0.833223 |              nan |    0.761748 |                 0 |               0 |               0 |       0.228524 |                             1 |\n",
      "|        18671 | 2006-03-31 00:00:00 |    400263           |             2 |      0 |       2 |       0.218823 |              nan |    0        |                 0 |               0 |               0 |       0        |                             0 |\n",
      "|        18671 | 2006-04-30 00:00:00 |         1.15e+06    |             1 |      0 |       1 |       0.906158 |              nan |    0.86594  |                 0 |               0 |               0 |       0.259782 |                             2 |\n",
      "|        18671 | 2006-06-30 00:00:00 |         1.08336e+06 |             1 |      0 |       1 |       0.997506 |              nan |    0.996437 |                 0 |               0 |               0 |       0.298931 |                             2 |\n"
     ]
    }
   ],
   "source": [
    "print(monthly.head().to_markdown(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| nonroutine_insider_quartile   | <function <lambda> at 0x1080c2fc0>   | <function <lambda> at 0x159153100>   | <function <lambda> at 0x159153a60>   | <function <lambda> at 0x1591536a0>   | <function <lambda> at 0x1080c2fc0>   | <function <lambda> at 0x159153100>   | <function <lambda> at 0x159153a60>   | <function <lambda> at 0x1591536a0>   | <function <lambda> at 0x1080c2fc0>   | <function <lambda> at 0x159153100>   | <function <lambda> at 0x159153a60>   | <function <lambda> at 0x1591536a0>   |\n",
      "|-------------------------------|--------------------------------------|--------------------------------------|--------------------------------------|--------------------------------------|--------------------------------------|--------------------------------------|--------------------------------------|--------------------------------------|--------------------------------------|--------------------------------------|--------------------------------------|--------------------------------------|\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# 1️. Merge “monthly” with your “merged” DataFrame (which contains excess returns)\n",
    "df = monthly.merge(merged, on=['COMPANY_ID','month_end'], how='left')\n",
    "print(df.head().to_markdown(index=False))\n",
    "# 2️. Create missing excess-return columns (as NaN)\n",
    "excess_cols = [f'excess_return_{h}m' for h in ['1','3','12']]\n",
    "for col in excess_cols:\n",
    "    if col not in df:\n",
    "        df[col] = np.nan\n",
    "\n",
    "# 3️. Drop rows where ALL excess returns are null\n",
    "valid = df.dropna(subset=excess_cols, how='all')\n",
    "print(f\"Dropped {len(df) - len(valid)} rows with no excess returns\")\n",
    "\n",
    "# 4️. Annualization helper\n",
    "def annualize(s):\n",
    "    s = s.dropna()\n",
    "    if s.empty:\n",
    "        return np.nan\n",
    "    n = len(s)\n",
    "    if n == 1:\n",
    "        return s.mean()*12\n",
    "    log_cum = np.log1p(s).sum()\n",
    "    ann = np.exp(log_cum * 12/n) - 1\n",
    "    return s.mean()*12 if np.isinf(ann) else ann\n",
    "\n",
    "# 5️. Group & aggregate\n",
    "group = valid.groupby('nonroutine_insider_quartile')\n",
    "results = pd.DataFrame({\n",
    "    'quartile': group.size().index,\n",
    "    **{\n",
    "        f'avg_excess_{h}m': group[f'excess_return_{h}m'].mean().values\n",
    "        for h in ['1','3','12']\n",
    "    },\n",
    "    **{\n",
    "        f'ann_excess_{h}m': group[f'excess_return_{h}m'].agg(annualize).values\n",
    "        for h in ['1','3','12']\n",
    "    },\n",
    "    **{\n",
    "        f'count_{h}m': group[f'excess_return_{h}m'].count().values\n",
    "        for h in ['1','3','12']\n",
    "    },\n",
    "    **{\n",
    "        f'hit_rate_{h}m': group[f'excess_return_{h}m'].apply(lambda x: (x>0).mean()).values\n",
    "        for h in ['1','3','12']\n",
    "    }\n",
    "})\n",
    "\n",
    "print(\"\\nPerformance metrics by nonroutine insider quartile:\")\n",
    "print(results.to_markdown(index=False))\n",
    "\n",
    "print(\"\"\"\n",
    "Notes:\n",
    "• Annualized returns use geometric compounding when possible; otherwise arithmetic mean × 12.\n",
    "• Hit rate = % of positive excess returns.\n",
    "• Count = number of non-null observations per horizon.\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ---------------------------\n",
    "# # Factor 2: Large Transaction\n",
    "# # ---------------------------\n",
    "# # Ensure trade_value exists; if not, compute it.\n",
    "# if 'trade_value' not in merged.columns:\n",
    "#     merged['trade_value'] = merged['TRANS_SHARES'] * merged['TRANS_PRICEPERSHARE']\n",
    "\n",
    "# # Sum trade value per individual (RPTOWNERNAME) per month.\n",
    "# individual_monthly_trade = merged.groupby(['COMPANY_ID', 'RPTOWNERNAME', 'month_end'])['trade_value'] \\\n",
    "#                                  .sum() \\\n",
    "#                                  .reset_index(name='total_trade_value')\n",
    "# # Compute each individual’s median total trade value over months.\n",
    "# individual_monthly_trade['median_trade_value'] = individual_monthly_trade.groupby(['COMPANY_ID', 'RPTOWNERNAME'])['total_trade_value'] \\\n",
    "#                                                                        .transform('median')\n",
    "# # Avoid division by zero; if median is 0, set ratio to NaN.\n",
    "# individual_monthly_trade['trade_value_ratio'] = np.where(\n",
    "#     individual_monthly_trade['median_trade_value'] > 0,\n",
    "#     individual_monthly_trade['total_trade_value'] / individual_monthly_trade['median_trade_value'],\n",
    "#     np.nan\n",
    "# )\n",
    "# # Flag abnormal if ratio exceeds threshold (e.g., 2).\n",
    "# individual_monthly_trade['score_large_transaction'] = np.where(individual_monthly_trade['trade_value_ratio'] > 2, 1, 0)\n",
    "# # Merge the individual large transaction score back to the main DataFrame.\n",
    "# merged = merged.merge(individual_monthly_trade[['COMPANY_ID', 'RPTOWNERNAME', 'month_end', 'score_large_transaction']],\n",
    "#                       on=['COMPANY_ID', 'RPTOWNERNAME', 'month_end'], how='left')\n",
    "\n",
    "# # ---------------------------\n",
    "# # Factor 3: Switch in Trading Direction\n",
    "# # ---------------------------\n",
    "# # Compute typical trade direction for each company (or consider doing it by individual).\n",
    "# typical_direction = merged.groupby('COMPANY_ID')['TRANS_CODE'].agg(\n",
    "#     lambda x: x.mode().iloc[0] if not x.mode().empty else np.nan\n",
    "# ).rename('typical_trade')\n",
    "# merged = merged.merge(typical_direction, left_on='COMPANY_ID', right_index=True, how='left')\n",
    "# # Now separate out the two possible switches:\n",
    "# merged['score_switch_buy_to_sell'] = np.where((merged['typical_trade'] == 'B') & (merged['TRANS_CODE'] == 'S'), 1, 0)\n",
    "# merged['score_switch_sell_to_buy'] = np.where((merged['typical_trade'] == 'S') & (merged['TRANS_CODE'] == 'B'), 1, 0)\n",
    "# # Optionally, you can combine these into a total switch flag:\n",
    "# merged['score_switch_direction'] = merged['score_switch_buy_to_sell'] + merged['score_switch_sell_to_buy']\n",
    "\n",
    "# # ---------------------------\n",
    "# # Factor 4: Abnormal Trade Volume\n",
    "# # ---------------------------\n",
    "# # Count unique traders per company per month using RPTOWNERNAME.\n",
    "# unique_traders = merged.groupby(['COMPANY_ID', 'month_end'])['RPTOWNERNAME'] \\\n",
    "#                        .nunique() \\\n",
    "#                        .reset_index(name='num_traders')\n",
    "# merged = merged.merge(unique_traders, on=['COMPANY_ID', 'month_end'], how='left')\n",
    "# # Compute each company’s average number of traders over time.\n",
    "# merged['avg_num_traders'] = merged.groupby('COMPANY_ID')['num_traders'] \\\n",
    "#                                   .transform('mean')\n",
    "# # Avoid division by zero.\n",
    "# merged['trade_volume_ratio'] = np.where(\n",
    "#     merged['avg_num_traders'] > 0,\n",
    "#     merged['num_traders'] / merged['avg_num_traders'],\n",
    "#     np.nan\n",
    "# )\n",
    "# # Flag abnormal if current month’s trader count is high (e.g., ratio > 1.5).\n",
    "# merged['score_trade_volume'] = np.where(merged['trade_volume_ratio'] > 1.5, 1, 0)\n",
    "\n",
    "# # ---------------------------\n",
    "# # Display a Sample of the Results\n",
    "# # ---------------------------\n",
    "# print(\"Sample of merged data with revised non-routine trading scores:\")\n",
    "# print(merged[['COMPANY_ID', 'month_end',\n",
    "#               'score_holdings_sale',\n",
    "#               'score_large_transaction',\n",
    "#               'score_switch_buy_to_sell',\n",
    "#               'score_switch_sell_to_buy',\n",
    "#               'score_trade_volume']].head().to_markdown(index=False))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
