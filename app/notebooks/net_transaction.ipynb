{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0: Retrieve Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory: /Users/atang/Documents/verdad/insider-transaction-research/app/notebooks\n",
      "Project root added to sys.path: /Users/atang/Documents/verdad/insider-transaction-research\n",
      "Both CSV files exist. Loading data from CSV...\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# Add the project root to sys.path (two levels up from app/notebooks)\n",
    "project_root = os.path.abspath(\"../..\")\n",
    "sys.path.append(project_root)\n",
    "\n",
    "print(\"Current working directory:\", os.getcwd())\n",
    "print(\"Project root added to sys.path:\", project_root)\n",
    "\n",
    "# Define the data directory at the project root\n",
    "data_dir = os.path.join(project_root, \"data\")\n",
    "if not os.path.exists(data_dir):\n",
    "    os.makedirs(data_dir)\n",
    "\n",
    "# Define paths for CSV files\n",
    "ndt_csv_path = os.path.join(data_dir, \"ndt.csv\")\n",
    "returns_csv_path = os.path.join(data_dir, \"returns_market_cap.csv\")\n",
    "\n",
    "# Flag to indicate if both CSV files exist\n",
    "data_loaded = False\n",
    "\n",
    "if os.path.exists(ndt_csv_path) and os.path.exists(returns_csv_path):\n",
    "    print(\"Both CSV files exist. Loading data from CSV...\")\n",
    "    ndt = pd.read_csv(ndt_csv_path, low_memory=False)\n",
    "    returns_and_market_cap = pd.read_csv(returns_csv_path)\n",
    "    data_loaded = True\n",
    "else:\n",
    "    print(\"At least one CSV file is missing.\")\n",
    "\n",
    "if not data_loaded:\n",
    "    from app.services.snowflake_query_executor import SnowflakeQueryExecutor\n",
    "\n",
    "    # Create a Snowflake query executor\n",
    "    print(\"Authorize Snowflake connection with Two-Factor Authentication...\")\n",
    "    executor = SnowflakeQueryExecutor(\n",
    "        user=os.getenv('USER'),\n",
    "        password=os.getenv('PASSWORD'),\n",
    "        account=os.getenv('ACCOUNT'),\n",
    "        warehouse=os.getenv('WAREHOUSE'),\n",
    "        database=os.getenv('DATABASE'),\n",
    "        schema=os.getenv('SCHEMA')\n",
    "    )\n",
    "\n",
    "    # Build SQL file paths (located inside app/sql/)\n",
    "    ndt_sql_query_file_path = os.path.join(project_root, \"app\", \"sql\", \"fetch_ndt.sql\")\n",
    "    market_cap_returns_sql_query_file_path = os.path.join(project_root, \"app\", \"sql\", \"fetch_returns_market_cap.sql\")\n",
    "\n",
    "    # Retrieve the non-derivative transactions data if missing\n",
    "    if not os.path.exists(ndt_csv_path):\n",
    "        print(\"Fetching non-derivative transactions data from Snowflake...\")\n",
    "        ndt = executor.execute_query_from_file(ndt_sql_query_file_path)\n",
    "        ndt.to_csv(ndt_csv_path, index=False)\n",
    "    else:\n",
    "        print(\"Non-derivative transactions CSV already exists.\")\n",
    "\n",
    "    # Retrieve market cap and returns data if missing\n",
    "    if not os.path.exists(returns_csv_path):\n",
    "        print(\"Fetching market cap and returns data from Snowflake...\")\n",
    "        returns_and_market_cap = executor.execute_query_from_file(market_cap_returns_sql_query_file_path)\n",
    "        returns_and_market_cap.to_csv(returns_csv_path, index=False)\n",
    "    else:\n",
    "        print(\"Market cap and returns CSV already exists.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Non-derivative transaction data:\n",
      "| ACCESSION_NUMBER     | TRANS_DATE   | TRANS_CODE   |   EQUITY_SWAP_INVOLVED |   TRANS_SHARES |   TRANS_PRICEPERSHARE | TRANS_ACQUIRED_DISP_CD   |   SHRS_OWND_FOLWNG_TRANS | DIRECT_INDIRECT_OWNERSHIP   |   COMPANY_ID | RPTOWNERNAME   | RPTOWNER_RELATIONSHIP   | RPTOWNER_TITLE                 |\n",
      "|:---------------------|:-------------|:-------------|-----------------------:|---------------:|----------------------:|:-------------------------|-------------------------:|:----------------------------|-------------:|:---------------|:------------------------|:-------------------------------|\n",
      "| 0000001750-06-000002 | 2006-01-04   | S            |                      0 |          50000 |                 24.64 | D                        |                 18810    | D                           |       168154 | STORCH DAVID P | Director,Officer        | Chairman, Pres., CEO, Director |\n",
      "| 0000001750-06-000002 | 2006-01-04   | S            |                      0 |          50000 |                 24.64 | D                        |                  6876.17 | D                           |       168154 | STORCH DAVID P | Director,Officer        | Chairman, Pres., CEO, Director |\n",
      "| 0000001750-06-000002 | 2006-01-05   | S            |                      0 |          22180 |                 24.94 | D                        |                 18810    | D                           |       168154 | STORCH DAVID P | Director,Officer        | Chairman, Pres., CEO, Director |\n",
      "| 0000001750-06-000002 | 2006-01-05   | M            |                      0 |          76302 |                 14.96 | A                        |                 18810    | D                           |       168154 | STORCH DAVID P | Director,Officer        | Chairman, Pres., CEO, Director |\n",
      "| 0000001750-06-000002 | 2006-01-05   | F            |                      0 |          59475 |                 25.34 | D                        |                 18810    | D                           |       168154 | STORCH DAVID P | Director,Officer        | Chairman, Pres., CEO, Director |\n",
      "\n",
      "Returns and market cap data:\n",
      "| MONTH_END   |   COMPANY_ID |   RETURN_LEAD_1_MONTHS |   RETURN_LEAD_3_MONTHS |   RETURN_LEAD_12_MONTHS |   MARKET_CAP_USD |\n",
      "|:------------|-------------:|-----------------------:|-----------------------:|------------------------:|-----------------:|\n",
      "| 1994-07-31  |        18511 |             0.106531   |              0.109966  |                0.375126 |          2616.76 |\n",
      "| 1994-08-31  |        18511 |             0.0139749  |              0.043478  |                0.255416 |          2885.57 |\n",
      "| 1994-09-30  |        18511 |            -0.0107207  |              0.0316986 |                0.261565 |          3001.81 |\n",
      "| 1994-10-31  |        18511 |             0.0402486  |              0.083469  |                0.292618 |          3078.85 |\n",
      "| 1994-11-30  |        18511 |             0.00252853 |              0.0355451 |                0.279064 |          3069.56 |\n"
     ]
    }
   ],
   "source": [
    "print(\"Non-derivative transaction data:\")\n",
    "print(ndt.head().to_markdown(index=False))\n",
    "print()\n",
    "print(\"Returns and market cap data:\")\n",
    "print(returns_and_market_cap.head().to_markdown(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1: Data Preparation and Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Dropped 15 rows with invalid TRANS_DATE.\n",
      "Sample of trades data after preparation:\n",
      "| ACCESSION_NUMBER     | TRANS_DATE          | TRANS_CODE   |   EQUITY_SWAP_INVOLVED |   TRANS_SHARES |   TRANS_PRICEPERSHARE | TRANS_ACQUIRED_DISP_CD   |   SHRS_OWND_FOLWNG_TRANS | DIRECT_INDIRECT_OWNERSHIP   |   COMPANY_ID | RPTOWNERNAME   | RPTOWNER_RELATIONSHIP   | RPTOWNER_TITLE                 | month_end           |      trade_value |\n",
      "|:---------------------|:--------------------|:-------------|-----------------------:|---------------:|----------------------:|:-------------------------|-------------------------:|:----------------------------|-------------:|:---------------|:------------------------|:-------------------------------|:--------------------|-----------------:|\n",
      "| 0000001750-06-000002 | 2006-01-04 00:00:00 | S            |                      0 |         144360 |                 24.64 | D                        |                 18810    | D                           |       168154 | STORCH DAVID P | Director,Officer        | Chairman, Pres., CEO, Director | 2006-01-31 00:00:00 |      3.55703e+06 |\n",
      "| 0000001750-06-000054 | 2006-03-28 00:00:00 | S            |                      0 |          66200 |                 27.38 | D                        |                  6876.17 | D                           |       168154 | STORCH DAVID P | Director,Officer        | President, CEO & Director      | 2006-03-31 00:00:00 |      1.81256e+06 |\n",
      "| 0000001750-06-000056 | 2006-03-29 00:00:00 | S            |                      0 |         290912 |                 27.44 | D                        |                 18810    | D                           |       168154 | STORCH DAVID P | Director,Officer        | President, CEO & Director      | 2006-03-31 00:00:00 |      7.98263e+06 |\n",
      "| 0000001750-06-000060 | 2006-03-31 00:00:00 | S            |                      0 |          50000 |                 28.29 | D                        |                  6876.17 | D                           |       168154 | STORCH DAVID P | Director,Officer        | President, CEO & Director      | 2006-03-31 00:00:00 |      1.4145e+06  |\n",
      "| 0000001750-06-000062 | 2006-04-03 00:00:00 | S            |                      0 |          24010 |                 29.03 | D                        |                  6876.17 | D                           |       168154 | STORCH DAVID P | Director,Officer        | President, CEO & Director      | 2006-04-30 00:00:00 | 697010           |\n"
     ]
    }
   ],
   "source": [
    "from pandas.tseries.offsets import MonthEnd\n",
    "\n",
    "# Convert date columns to datetime for consistent handling\n",
    "# MONTH_END in returns_and_market_cap is the end-of-month date for returns/market cap\n",
    "returns_and_market_cap['MONTH_END'] = pd.to_datetime(returns_and_market_cap['MONTH_END'])\n",
    "\n",
    "# TRANS_DATE in ndt is the transaction date; use errors='coerce' to handle invalid dates\n",
    "ndt['TRANS_DATE'] = pd.to_datetime(ndt['TRANS_DATE'], errors='coerce')\n",
    "\n",
    "# Create a month-end column in ndt by rounding transaction dates to the nearest month end\n",
    "ndt['month_end'] = ndt['TRANS_DATE'] + MonthEnd(0)\n",
    "\n",
    "# Check for and drop rows with invalid transaction dates (NaT)\n",
    "invalid_dates = ndt['TRANS_DATE'].isna().sum()\n",
    "if invalid_dates > 0:\n",
    "    print(f\"Warning: Dropped {invalid_dates} rows with invalid TRANS_DATE.\")\n",
    "    ndt = ndt.dropna(subset=['TRANS_DATE'])\n",
    "\n",
    "# Filter ndt to include only buy ('P') and sell ('S') transactions\n",
    "trades = ndt[ndt['TRANS_CODE'].isin(['P', 'S'])].copy()\n",
    "\n",
    "# Collapse duplicate rows so each ACCESSION_NUMBER is one trade\n",
    "trades = trades.groupby('ACCESSION_NUMBER', as_index=False).agg({\n",
    "    'TRANS_DATE': 'first',\n",
    "    'TRANS_CODE': 'first',\n",
    "    'EQUITY_SWAP_INVOLVED': 'first',\n",
    "    'TRANS_SHARES': 'sum',\n",
    "    'TRANS_PRICEPERSHARE': 'first',\n",
    "    'TRANS_ACQUIRED_DISP_CD': 'first',\n",
    "    'SHRS_OWND_FOLWNG_TRANS': 'first',\n",
    "    'DIRECT_INDIRECT_OWNERSHIP': 'first',\n",
    "    'COMPANY_ID': 'first',\n",
    "    'RPTOWNERNAME': 'first',\n",
    "    'RPTOWNER_RELATIONSHIP': 'first',\n",
    "    'RPTOWNER_TITLE': 'first',\n",
    "    'month_end': 'first'\n",
    "})\n",
    "\n",
    "# Calculate the dollar value of each trade (shares * price per share)\n",
    "trades['trade_value'] = trades['TRANS_SHARES'] * trades['TRANS_PRICEPERSHARE']\n",
    "\n",
    "print(\"Sample of trades data after preparation:\")\n",
    "print(trades.head().to_markdown(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5: Residualize Trade Value on Market Cap "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample of trades data with residualized trade value (trade_value_res):\n",
      "| ACCESSION_NUMBER     | TRANS_DATE          | TRANS_CODE   |   EQUITY_SWAP_INVOLVED |   TRANS_SHARES |   TRANS_PRICEPERSHARE | TRANS_ACQUIRED_DISP_CD   |   SHRS_OWND_FOLWNG_TRANS | DIRECT_INDIRECT_OWNERSHIP   |   COMPANY_ID | RPTOWNERNAME   | RPTOWNER_RELATIONSHIP   | RPTOWNER_TITLE                 | month_end           |      trade_value | MONTH_END           |   MARKET_CAP_USD |   log_trade_value |   log_market_cap |   trade_value_res |\n",
      "|:---------------------|:--------------------|:-------------|-----------------------:|---------------:|----------------------:|:-------------------------|-------------------------:|:----------------------------|-------------:|:---------------|:------------------------|:-------------------------------|:--------------------|-----------------:|:--------------------|-----------------:|------------------:|-----------------:|------------------:|\n",
      "| 0000001750-06-000002 | 2006-01-04 00:00:00 | S            |                      0 |         144360 |                 24.64 | D                        |                 18810    | D                           |       168154 | STORCH DAVID P | Director,Officer        | Chairman, Pres., CEO, Director | 2006-01-31 00:00:00 |      3.55703e+06 | 2006-01-31 00:00:00 |          798.269 |           15.0844 |          6.68245 |           3.02156 |\n",
      "| 0000001750-06-000054 | 2006-03-28 00:00:00 | S            |                      0 |          66200 |                 27.38 | D                        |                  6876.17 | D                           |       168154 | STORCH DAVID P | Director,Officer        | President, CEO & Director      | 2006-03-31 00:00:00 |      1.81256e+06 | 2006-03-31 00:00:00 |         1035.22  |           14.4102 |          6.94237 |           2.19248 |\n",
      "| 0000001750-06-000056 | 2006-03-29 00:00:00 | S            |                      0 |         290912 |                 27.44 | D                        |                 18810    | D                           |       168154 | STORCH DAVID P | Director,Officer        | President, CEO & Director      | 2006-03-31 00:00:00 |      7.98263e+06 | 2006-03-31 00:00:00 |         1035.22  |           15.8928 |          6.94237 |           3.67501 |\n",
      "| 0000001750-06-000060 | 2006-03-31 00:00:00 | S            |                      0 |          50000 |                 28.29 | D                        |                  6876.17 | D                           |       168154 | STORCH DAVID P | Director,Officer        | President, CEO & Director      | 2006-03-31 00:00:00 |      1.4145e+06  | 2006-03-31 00:00:00 |         1035.22  |           14.1623 |          6.94237 |           1.94452 |\n",
      "| 0000001750-06-000062 | 2006-04-03 00:00:00 | S            |                      0 |          24010 |                 29.03 | D                        |                  6876.17 | D                           |       168154 | STORCH DAVID P | Director,Officer        | President, CEO & Director      | 2006-04-30 00:00:00 | 697010           | 2006-04-30 00:00:00 |          972.783 |           13.4546 |          6.88016 |           1.27386 |\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# Merge trades with market cap data from returns_and_market_cap on both COMPANY_ID and month_end date\n",
    "trades_merged = trades.merge(\n",
    "    returns_and_market_cap[['COMPANY_ID', 'MONTH_END', 'MARKET_CAP_USD']],\n",
    "    left_on=['COMPANY_ID', 'month_end'],\n",
    "    right_on=['COMPANY_ID', 'MONTH_END'],\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Drop rows where market cap or trade value is missing\n",
    "trades_merged = trades_merged.dropna(subset=['MARKET_CAP_USD', 'trade_value'])\n",
    "\n",
    "# Ensure the date columns are datetime\n",
    "trades_merged['month_end'] = pd.to_datetime(trades_merged['month_end'])\n",
    "trades_merged['MONTH_END'] = pd.to_datetime(trades_merged['MONTH_END'])\n",
    "\n",
    "# Filter out rows with non-positive values to avoid log issues\n",
    "trades_merged = trades_merged[(trades_merged['trade_value'] > 0) & (trades_merged['MARKET_CAP_USD'] > 0)]\n",
    "\n",
    "# Create log-transformed variables\n",
    "trades_merged['log_trade_value'] = np.log(trades_merged['trade_value'])\n",
    "trades_merged['log_market_cap'] = np.log(trades_merged['MARKET_CAP_USD'])\n",
    "\n",
    "# Prepare regression variables with a constant for the intercept\n",
    "X = sm.add_constant(trades_merged['log_market_cap'])\n",
    "y = trades_merged['log_trade_value']\n",
    "\n",
    "# Fit the regression model\n",
    "model = sm.OLS(y, X).fit()\n",
    "\n",
    "# Calculate the residualized trade value (on the log scale)\n",
    "trades_merged['trade_value_res'] = model.resid\n",
    "\n",
    "# Display a sample of the results\n",
    "print(\"Sample of trades data with residualized trade value (trade_value_res):\")\n",
    "print(trades_merged.head().to_markdown(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2: Market Returns Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample of market_return_1m market returns:\n",
      "| MONTH_END           |   market_return_1m |\n",
      "|:--------------------|-------------------:|\n",
      "| 1962-01-31 00:00:00 |          0.0886072 |\n",
      "| 1962-02-28 00:00:00 |         -0.0697672 |\n",
      "| 1962-03-31 00:00:00 |          0.0249994 |\n",
      "| 1962-04-30 00:00:00 |         -0.414634  |\n",
      "| 1962-05-31 00:00:00 |          0.0520832 |\n",
      "Sample of market_return_3m market returns:\n",
      "| MONTH_END           |   market_return_3m |\n",
      "|:--------------------|-------------------:|\n",
      "| 1962-01-31 00:00:00 |           0.037974 |\n",
      "| 1962-02-28 00:00:00 |          -0.44186  |\n",
      "| 1962-03-31 00:00:00 |          -0.36875  |\n",
      "| 1962-04-30 00:00:00 |          -0.256097 |\n",
      "| 1962-05-31 00:00:00 |           0.375    |\n",
      "Sample of market_return_12m market returns:\n",
      "| MONTH_END           |   market_return_12m |\n",
      "|:--------------------|--------------------:|\n",
      "| 1962-01-31 00:00:00 |           -0.319621 |\n",
      "| 1962-02-28 00:00:00 |           -0.331395 |\n",
      "| 1962-03-31 00:00:00 |           -0.234376 |\n",
      "| 1962-04-30 00:00:00 |           -0.14939  |\n",
      "| 1962-05-31 00:00:00 |            0.510417 |\n",
      "Sample of final DataFrame with excess returns:\n",
      "| MONTH_END           |   COMPANY_ID |   RETURN_LEAD_1_MONTHS |   RETURN_LEAD_3_MONTHS |   RETURN_LEAD_12_MONTHS |   MARKET_CAP_USD |      mkt_1 |       mkt_3 |   mkt_12 |   excess_return_1m |   excess_return_3m |   excess_return_12m |   SIZE_QUARTILE |\n",
      "|:--------------------|-------------:|-----------------------:|-----------------------:|------------------------:|-----------------:|-----------:|------------:|---------:|-------------------:|-------------------:|--------------------:|----------------:|\n",
      "| 1994-07-31 00:00:00 |        18511 |             0.106531   |              0.109966  |                0.375126 |          2616.76 |  0.0356084 |  0.0250432  | 0.130296 |          0.0709226 |          0.0849229 |           0.24483   |               4 |\n",
      "| 1994-08-31 00:00:00 |        18511 |             0.0139749  |              0.043478  |                0.255416 |          2885.57 | -0.0255001 | -0.0437436  | 0.114291 |          0.039475  |          0.0872216 |           0.141125  |               4 |\n",
      "| 1994-09-30 00:00:00 |        18511 |            -0.0107207  |              0.0316986 |                0.261565 |          3001.81 |  0.0149824 | -0.0062338  | 0.17374  |         -0.0257031 |          0.0379324 |           0.0878244 |               4 |\n",
      "| 1994-10-31 00:00:00 |        18511 |             0.0402486  |              0.083469  |                0.292618 |          3078.85 | -0.0333065 | -0.0382963  | 0.144662 |          0.0735551 |          0.121765  |           0.147956  |               4 |\n",
      "| 1994-11-30 00:00:00 |        18511 |             0.00252853 |              0.0355451 |                0.279064 |          3069.56 |  0.012939  |  0.00780549 | 0.233204 |         -0.0104105 |          0.0277396 |           0.04586   |               4 |\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Assume returns_and_market_cap is your DataFrame with columns:\n",
    "# 'MONTH_END', 'MARKET_CAP_USD', 'RETURN_LEAD_1_MONTHS', \n",
    "# 'RETURN_LEAD_3_MONTHS', 'RETURN_LEAD_12_MONTHS'\n",
    "\n",
    "# Drop rows with missing MARKET_CAP_USD for a clean weighting process\n",
    "df = returns_and_market_cap.dropna(subset=['MARKET_CAP_USD']).copy()\n",
    "\n",
    "# Compute market-cap weighted returns per MONTH_END using np.average for each horizon\n",
    "weighted_returns = df.groupby('MONTH_END').apply(\n",
    "    lambda group: pd.Series({\n",
    "        'mkt_1': np.average(group['RETURN_LEAD_1_MONTHS'], weights=group['MARKET_CAP_USD']),\n",
    "        'mkt_3': np.average(group['RETURN_LEAD_3_MONTHS'], weights=group['MARKET_CAP_USD']),\n",
    "        'mkt_12': np.average(group['RETURN_LEAD_12_MONTHS'], weights=group['MARKET_CAP_USD']),\n",
    "    }),\n",
    "    include_groups=False\n",
    ").reset_index()\n",
    "\n",
    "# Build a dictionary of market returns, renaming columns to match your original naming\n",
    "market_returns = {}\n",
    "for horizon in ['1', '3', '12']:\n",
    "    col = 'mkt_' + horizon\n",
    "    market_returns[f'market_return_{horizon}m'] = (\n",
    "        weighted_returns[['MONTH_END', col]]\n",
    "        .rename(columns={col: f'market_return_{horizon}m'})\n",
    "    )\n",
    "\n",
    "# Print sample market returns for each horizon\n",
    "for horizon in ['1', '3', '12']:\n",
    "    key = f'market_return_{horizon}m'\n",
    "    print(f\"Sample of {key} market returns:\")\n",
    "    print(market_returns[key].head().to_markdown(index=False))\n",
    "\n",
    "# Merge the weighted market returns back into the main DataFrame\n",
    "df = df.merge(weighted_returns, on='MONTH_END', how='left')\n",
    "\n",
    "# Calculate excess returns: individual returns minus the market weighted return\n",
    "df['excess_return_1m']  = df['RETURN_LEAD_1_MONTHS']  - df['mkt_1']\n",
    "df['excess_return_3m']  = df['RETURN_LEAD_3_MONTHS']  - df['mkt_3']\n",
    "df['excess_return_12m'] = df['RETURN_LEAD_12_MONTHS'] - df['mkt_12']\n",
    "\n",
    "# Create size quartiles based on MARKET_CAP_USD\n",
    "df['SIZE_QUARTILE'] = pd.qcut(df['MARKET_CAP_USD'], q=4, labels=[1, 2, 3, 4])\n",
    "\n",
    "# Convert MONTH_END to datetime (if needed)\n",
    "df['MONTH_END'] = pd.to_datetime(df['MONTH_END'], errors='coerce')\n",
    "\n",
    "return_df_final = df\n",
    "\n",
    "# Print a sample of the final DataFrame with excess returns\n",
    "print(\"Sample of final DataFrame with excess returns:\")\n",
    "print(return_df_final.head().to_markdown(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3: Merging Trades with Returns Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample of merged data with excess returns:\n",
      "| ACCESSION_NUMBER     | TRANS_DATE          | TRANS_CODE   |   EQUITY_SWAP_INVOLVED |   TRANS_SHARES |   TRANS_PRICEPERSHARE | TRANS_ACQUIRED_DISP_CD   |   SHRS_OWND_FOLWNG_TRANS | DIRECT_INDIRECT_OWNERSHIP   |   COMPANY_ID | RPTOWNERNAME   | RPTOWNER_RELATIONSHIP   | RPTOWNER_TITLE                 | month_end           |      trade_value | MONTH_END           |   MARKET_CAP_USD_x |   log_trade_value |   log_market_cap |   trade_value_res |   RETURN_LEAD_1_MONTHS |   RETURN_LEAD_3_MONTHS |   RETURN_LEAD_12_MONTHS |   MARKET_CAP_USD_y |       mkt_1 |      mkt_3 |   mkt_12 |   excess_return_1m |   excess_return_3m |   excess_return_12m |   SIZE_QUARTILE |\n",
      "|:---------------------|:--------------------|:-------------|-----------------------:|---------------:|----------------------:|:-------------------------|-------------------------:|:----------------------------|-------------:|:---------------|:------------------------|:-------------------------------|:--------------------|-----------------:|:--------------------|-------------------:|------------------:|-----------------:|------------------:|-----------------------:|-----------------------:|------------------------:|-------------------:|------------:|-----------:|---------:|-------------------:|-------------------:|--------------------:|----------------:|\n",
      "| 0000001750-06-000002 | 2006-01-04 00:00:00 | S            |                      0 |         144360 |                 24.64 | D                        |                 18810    | D                           |       168154 | STORCH DAVID P | Director,Officer        | Chairman, Pres., CEO, Director | 2006-01-31 00:00:00 |      3.55703e+06 | 2006-01-31 00:00:00 |            798.269 |           15.0844 |          6.68245 |           3.02156 |              0.0621066 |               0.119597 |               0.250105  |            798.269 |  0.00362027 |  0.0450093 | 0.163101 |          0.0584863 |          0.0745878 |           0.0870041 |               3 |\n",
      "| 0000001750-06-000054 | 2006-03-28 00:00:00 | S            |                      0 |          66200 |                 27.38 | D                        |                  6876.17 | D                           |       168154 | STORCH DAVID P | Director,Officer        | President, CEO & Director      | 2006-03-31 00:00:00 |      1.81256e+06 | 2006-03-31 00:00:00 |           1035.22  |           14.4102 |          6.94237 |           2.19248 |             -0.0632022 |              -0.219452 |              -0.0323034 |           1035.22  |  0.0120955  | -0.0260301 | 0.132739 |         -0.0752978 |         -0.193422  |          -0.165043  |               3 |\n",
      "| 0000001750-06-000056 | 2006-03-29 00:00:00 | S            |                      0 |         290912 |                 27.44 | D                        |                 18810    | D                           |       168154 | STORCH DAVID P | Director,Officer        | President, CEO & Director      | 2006-03-31 00:00:00 |      7.98263e+06 | 2006-03-31 00:00:00 |           1035.22  |           15.8928 |          6.94237 |           3.67501 |             -0.0632022 |              -0.219452 |              -0.0323034 |           1035.22  |  0.0120955  | -0.0260301 | 0.132739 |         -0.0752978 |         -0.193422  |          -0.165043  |               3 |\n",
      "| 0000001750-06-000060 | 2006-03-31 00:00:00 | S            |                      0 |          50000 |                 28.29 | D                        |                  6876.17 | D                           |       168154 | STORCH DAVID P | Director,Officer        | President, CEO & Director      | 2006-03-31 00:00:00 |      1.4145e+06  | 2006-03-31 00:00:00 |           1035.22  |           14.1623 |          6.94237 |           1.94452 |             -0.0632022 |              -0.219452 |              -0.0323034 |           1035.22  |  0.0120955  | -0.0260301 | 0.132739 |         -0.0752978 |         -0.193422  |          -0.165043  |               3 |\n",
      "| 0000001750-06-000062 | 2006-04-03 00:00:00 | S            |                      0 |          24010 |                 29.03 | D                        |                  6876.17 | D                           |       168154 | STORCH DAVID P | Director,Officer        | President, CEO & Director      | 2006-04-30 00:00:00 | 697010           | 2006-04-30 00:00:00 |            972.783 |           13.4546 |          6.88016 |           1.27386 |             -0.0974513 |              -0.112444 |               0.144678  |            972.783 | -0.0429758  | -0.0345301 | 0.155928 |         -0.0544755 |         -0.0779137 |          -0.0112499 |               3 |\n"
     ]
    }
   ],
   "source": [
    "# Merge trades with returns_and_market_cap to align trade data with returns and market cap\n",
    "# Use left join to keep all trades, even if no matching returns data exists\n",
    "merged = trades_merged.merge(return_df_final, \n",
    "                      left_on=['COMPANY_ID', 'MONTH_END'], \n",
    "                      right_on=['COMPANY_ID', 'MONTH_END'], \n",
    "                      how='left')#.drop(columns='MONTH_END')  # Drop redundant MONTH_END column\n",
    "\n",
    "print(\"Sample of merged data with excess returns:\")\n",
    "print(merged.head().to_markdown(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4: Net Trade Value and Quintile Assignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net trade value and quintile assignment elapsed: 0.1833 seconds\n",
      "Sample of merged data with net trade value and quintiles:\n",
      "|   COMPANY_ID | month_end           |   net_trade_value |   net_value_quintile |\n",
      "|-------------:|:--------------------|------------------:|---------------------:|\n",
      "|       168154 | 2006-01-31 00:00:00 |          -3.02156 |                    3 |\n",
      "|       168154 | 2006-03-31 00:00:00 |          -7.81201 |                    1 |\n",
      "|       168154 | 2006-03-31 00:00:00 |          -7.81201 |                    1 |\n",
      "|       168154 | 2006-03-31 00:00:00 |          -7.81201 |                    1 |\n",
      "|       168154 | 2006-04-30 00:00:00 |          -1.27386 |                    3 |\n",
      "net_value_quintile legend:\n",
      "1 = lowest net trade value (biggest net sellers)\n",
      "2 = lower-middle net trade value\n",
      "3 = middle net trade value\n",
      "4 = upper-middle net trade value\n",
      "5 = highest net trade value (biggest net buyers)\n"
     ]
    }
   ],
   "source": [
    "import time \n",
    "\n",
    "# Start timing this section for performance tracking\n",
    "start = time.perf_counter()\n",
    "\n",
    "# Compute total trade value by company, month, and transaction type (P or S) in one pass\n",
    "# Use unstack to pivot P and S into columns, filling missing values with 0\n",
    "trade_values = trades_merged.groupby(['COMPANY_ID', 'month_end', 'TRANS_CODE'])['trade_value_res'].sum().unstack(fill_value=0)\n",
    "\n",
    "# Calculate net trade value as buys (P) minus sells (S)\n",
    "# Use .get() to safely handle cases where 'P' or 'S' might be missing\n",
    "net_value = (trade_values.get('P', 0) - trade_values.get('S', 0)).reset_index(name='net_trade_value')\n",
    "\n",
    "# Merge net trade value back into the merged DataFrame\n",
    "merged = merged.merge(net_value, on=['COMPANY_ID', 'month_end'], how='left')\n",
    "\n",
    "# Define a helper function to assign quintiles robustly\n",
    "# Forces 5 bins using qcut, falls back to equal-range bins if necessary\n",
    "def safe_qcut(x, q=5):\n",
    "    try:\n",
    "        # Attempt to create 5 quantile-based bins\n",
    "        return pd.qcut(x, q, labels=False, duplicates='drop') + 1\n",
    "    except ValueError:\n",
    "        # If qcut fails (e.g., too few unique values), use equal-range bins\n",
    "        if x.notna().nunique() > 1:\n",
    "            return pd.cut(x, bins=min(q, x.notna().nunique()), labels=False, include_lowest=True) + 1\n",
    "        # If only one unique value or all NaN, assign to quintile 1\n",
    "        return pd.Series(1, index=x.index)\n",
    "\n",
    "# Assign quintiles based on net_trade_value within each month_end\n",
    "merged['net_value_quintile'] = merged.groupby('month_end')['net_trade_value'].transform(safe_qcut)\n",
    "\n",
    "print(f\"Net trade value and quintile assignment elapsed: {time.perf_counter() - start:.4f} seconds\")\n",
    "print(\"Sample of merged data with net trade value and quintiles:\")\n",
    "print(merged[['COMPANY_ID', 'month_end', 'net_trade_value', 'net_value_quintile']].head().to_markdown(index=False))\n",
    "print(\n",
    "    \"net_value_quintile legend:\\n\"\n",
    "    \"1 = lowest net trade value (biggest net sellers)\\n\"\n",
    "    \"2 = lower-middle net trade value\\n\"\n",
    "    \"3 = middle net trade value\\n\"\n",
    "    \"4 = upper-middle net trade value\\n\"\n",
    "    \"5 = highest net trade value (biggest net buyers)\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5: Performance Analysis of Net Trade Value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropped 0 rows due to missing excess returns.\n",
      "Performance by net trade value quintile:\n",
      "|    |   net_value_quintile |   ann_excess_1m |   ann_excess_3m |   ann_excess_12m |   hit_rate_1m |   hit_rate_3m |   hit_rate_2m |   count_1m |   count_3m |   count_12m |\n",
      "|---:|---------------------:|----------------:|----------------:|-----------------:|--------------:|--------------:|--------------:|-----------:|-----------:|------------:|\n",
      "|  0 |                    1 |      0.0315202  |       0.163099  |         0.100343 |      0.516923 |      0.474913 |      0.406988 |        264 |        264 |         261 |\n",
      "|  1 |                    2 |      0.00909527 |       0.0315588 |         0.117719 |      0.491279 |      0.477047 |      0.46343  |        241 |        241 |         238 |\n",
      "|  2 |                    3 |      0.0427328  |       0.0180076 |         0.224038 |      0.489858 |      0.481519 |      0.466862 |        249 |        249 |         246 |\n",
      "|  3 |                    4 |     -0.0215705  |       0.0220388 |         0.278261 |      0.491322 |      0.481878 |      0.473371 |        241 |        241 |         238 |\n",
      "|  4 |                    5 |      0.0597842  |       0.104845  |         0.420737 |      0.481908 |      0.473152 |      0.462449 |        248 |        248 |         245 |\n",
      "\n",
      "Notes:\n",
      "- Annualized returns use geometric compounding; non-positive returns fall back to arithmetic mean * 12.\n",
      "- Hit rate measures the fraction of months where excess return > 0.\n",
      "- Counts show the avg number of monthly observations per quintile for each horizon.\n"
     ]
    }
   ],
   "source": [
    "# Filter to rows with at least one non-null excess return (more permissive than all three)\n",
    "valid = merged.dropna(subset=[f'excess_return_{h}m' for h in ['1', '3', '12']], how='all').copy()\n",
    "print(f\"Dropped {len(merged) - len(valid)} rows due to missing excess returns.\")\n",
    "\n",
    "# Compute monthly average excess returns by quintile\n",
    "monthly = valid.groupby(['month_end', 'net_value_quintile'])[[f'excess_return_{h}m' for h in ['1', '3', '12']]].mean().reset_index()\n",
    "\n",
    "# Define a function to annualize returns\n",
    "# Uses geometric compounding; falls back to arithmetic mean if cumulative return is non-positive\n",
    "def annualize(series):\n",
    "    series = series.dropna()  # Drop NaNs before calculation\n",
    "    cum_return = (1 + series).prod()  # Cumulative return\n",
    "    n = len(series)  # Number of periods\n",
    "    # If positive cumulative return and data exists, annualize geometrically\n",
    "    if cum_return > 0 and n > 0:\n",
    "        return cum_return**(12/n) - 1\n",
    "    # Otherwise, use arithmetic mean annualized (or NaN if no data)\n",
    "    return series.mean() * 12 if n > 0 else np.nan\n",
    "\n",
    "# Calculate annualized excess returns per quintile\n",
    "agg = monthly.groupby('net_value_quintile').agg(**{\n",
    "    f'ann_excess_{h}m': (f'excess_return_{h}m', annualize) for h in ['1', '3', '12']\n",
    "}).reset_index()\n",
    "\n",
    "# Count non-null monthly observations per quintile for each horizon\n",
    "counts = monthly.groupby('net_value_quintile').agg(**{\n",
    "    f'count_{h}m': (f'excess_return_{h}m', 'count') for h in ['1', '3', '12']\n",
    "}).reset_index()\n",
    "\n",
    "# Calculate hit rate: proportion of months where excess return > 0 (positive performance)\n",
    "hit = valid.groupby('month_end').apply(\n",
    "    lambda df: df.assign(**{f'beat_mean_{h}m': df[f'excess_return_{h}m'] > 0 for h in ['1', '3', '12']}),\n",
    "    include_groups=False  # Exclude group keys from apply\n",
    ").groupby('net_value_quintile')[[f'beat_mean_{h}m' for h in ['1', '3', '12']]].mean().reset_index().rename(\n",
    "    columns=lambda c: f'hit_rate_{c[-2:]}' if 'beat' in c else c\n",
    ")\n",
    "\n",
    "# Combine all performance metrics into one DataFrame\n",
    "performance = agg.merge(hit, on='net_value_quintile').merge(counts, on='net_value_quintile').sort_values('net_value_quintile')\n",
    "\n",
    "print(\"Performance by net trade value quintile:\")\n",
    "print(performance.to_markdown())\n",
    "\n",
    "print(\"\\nNotes:\")\n",
    "print(\"- Annualized returns use geometric compounding; non-positive returns fall back to arithmetic mean * 12.\")\n",
    "print(\"- Hit rate measures the fraction of months where excess return > 0.\")\n",
    "print(\"- Counts show the avg number of monthly observations per quintile for each horizon.\")\n",
    "\n",
    "# when you have fat tailed returns the mean is shifted higher \n",
    "# when you have returns that aren't logged you will overestimate the return because you mean is dragged to the right\n",
    "# compound over time go down 20 and back 20 to get the return "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyisis:\n",
    "- Best short‑term performance (1–3m) comes from the largest net buyers (Quintile 1).\n",
    "- Worst performance is clearly in Quintile 5 (biggest net sellers).\n",
    "- Over 12 months, middle quintiles (2–3) still outperform extreme sellers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import ttest_1samp\n",
    "\n",
    "# Filter valid observations\n",
    "valid = valid.copy()\n",
    "\n",
    "results = []\n",
    "for quintile, df in valid.groupby('net_value_quintile'):\n",
    "    for horizon in ['1', '3', '12']:\n",
    "        series = df[f'excess_return_{horizon}m'].dropna()\n",
    "        if len(series) < 5:\n",
    "            # Skip tiny samples\n",
    "            continue\n",
    "        t_stat, p_value = ttest_1samp(series, 0)\n",
    "        results.append({\n",
    "            'net_value_quintile': quintile,\n",
    "            'horizon_months': int(horizon),\n",
    "            'mean_excess_return': series.mean(),\n",
    "            't_stat': t_stat,\n",
    "            'p_value': p_value,\n",
    "            'n_obs': len(series)\n",
    "        })\n",
    "\n",
    "ttest_df = pd.DataFrame(results).sort_values(['net_value_quintile','horizon_months'])\n",
    "\n",
    "print(\"One-sample t-test of excess returns vs zero by quintile:\")\n",
    "print(ttest_df.to_markdown(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assign monthly size quintiles to market cap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure that the merged dataframe has a size_quintile assignment based on MARKET_CAP_USD for each month\n",
    "merged['size_quintile'] = (\n",
    "    merged\n",
    "    .groupby('month_end')['MARKET_CAP_USD']\n",
    "    .transform(lambda x: pd.qcut(x, 5, labels=False, duplicates='drop') + 1)\n",
    ")\n",
    "\n",
    "# (Optional) Recreate the valid dataset if needed so that it carries over the new column.\n",
    "valid = merged.dropna(subset=[f'excess_return_{h}m' for h in ['1', '3', '12']], how='all').copy()\n",
    "\n",
    "print(\"Sample of valid data with size_quintile assigned:\")\n",
    "print(valid[['COMPANY_ID', 'month_end', 'MARKET_CAP_USD', 'size_quintile']].head().to_markdown(index=False))\n",
    "print(\n",
    "    \"size_quintile legend:\\n\"\n",
    "    \"1 = smallest 20% of companies by market cap\\n\"\n",
    "    \"2 = next 20%\\n\"\n",
    "    \"3 = middle 20%\\n\"\n",
    "    \"4 = next 20%\\n\"\n",
    "    \"5 = largest 20% of companies by market cap\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute performance by (size × net‑value) buckets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1️. Ensure size quintiles are assigned in the merged dataframe\n",
    "merged['size_quintile'] = (\n",
    "    merged\n",
    "    .groupby('month_end')['MARKET_CAP_USD']\n",
    "    .transform(lambda x: pd.qcut(x, 5, labels=False, duplicates='drop') + 1)\n",
    ")\n",
    "\n",
    "# Recreate the valid dataset so it carries the new 'size_quintile'\n",
    "valid = merged.dropna(subset=[f'excess_return_{h}m' for h in ['1', '3', '12']], how='all').copy()\n",
    "\n",
    "# 2️. Aggregate monthly excess returns by month, size_quintile, and net_value_quintile.\n",
    "monthly_double = (\n",
    "    valid.groupby(['month_end', 'size_quintile', 'net_value_quintile'])[\n",
    "        [f'excess_return_{h}m' for h in ['1', '3', '12']]\n",
    "    ]\n",
    "    .mean()\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "# Function to annualize returns (geometric compounding or arithmetic fallback)\n",
    "def annualize(series):\n",
    "    series = series.dropna()  # Remove missing values\n",
    "    cum_return = (1 + series).prod()  # Cumulative return\n",
    "    n = len(series)  # Number of periods\n",
    "    if cum_return > 0 and n > 0:\n",
    "        return cum_return**(12/n) - 1  # Geometric annualization\n",
    "    return series.mean() * 12 if n > 0 else np.nan  # Fallback: arithmetic\n",
    "\n",
    "# Aggregate performance metrics for each double-sorted bucket\n",
    "def agg_perf(df):\n",
    "    return pd.Series({\n",
    "        **{f'ann_excess_{h}m': annualize(df[f'excess_return_{h}m']) for h in ['1', '3', '12']},\n",
    "        **{f'hit_rate_{h}m': (df[f'excess_return_{h}m'] > 0).mean() for h in ['1', '3', '12']},\n",
    "        'n_obs': len(df)  # Number of monthly observations in this bucket\n",
    "    })\n",
    "\n",
    "# Group by size and net trade quintile, applying agg_perf while excluding grouping columns (fixes deprecation warning)\n",
    "double_perf = (\n",
    "    monthly_double\n",
    "    .groupby(['size_quintile', 'net_value_quintile'])\n",
    "    .apply(agg_perf, include_groups=False)\n",
    "    .reset_index()\n",
    "    .sort_values(['size_quintile', 'net_value_quintile'])\n",
    ")\n",
    "\n",
    "print(\"Double-sorted performance (by size quintile and net trade quintile):\")\n",
    "print(double_perf.to_markdown(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Hit rate ~50% across almost every bucket means none consistently beat the market far more often than they lose. \n",
    "- Small stocks that sold the most (size_quintile 1 & net_value_quintile 1) had the strongest short‑term (1‑month) and especially long‑term (12‑month) excess returns.\n",
    "- Big buyers (net_value_quintile 5) tended to underperform the market, especially in small firms.\n",
    "- Some medium‑large buckets (size 5, net_value_quintile 2–3) show modest positive returns and hit rates above 55% over 12 months.\n",
    "\n",
    "##### Overall, selling activity in small companies appears linked to higher future returns, but the pattern weakens for larger firms and isn’t ironclad (hit rates hover around 50%)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## pre‑2015 vs post‑2015 Does the “buyer premium” hold throughout?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a new column \"period\" to indicate pre‑2015 vs post‑2015\n",
    "valid['period'] = np.where(valid['month_end'] < pd.Timestamp('2015-01-01'), 'Pre-2015', 'Post-2015')\n",
    "\n",
    "# Check how many observations are in each period\n",
    "print(\"Observations by period:\")\n",
    "print(valid['period'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute Performance Metrics by Period and Net Trade Quintile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate monthly average excess returns by month, period, and net trade quintile\n",
    "monthly_period = valid.groupby(['month_end', 'period', 'net_value_quintile'])[\n",
    "    [f'excess_return_{h}m' for h in ['1', '3', '12']]\n",
    "].mean().reset_index()\n",
    "\n",
    "# Use the same annualization function as before\n",
    "def annualize(series):\n",
    "    series = series.dropna()  # Remove missing values\n",
    "    cum_return = (1 + series).prod()  # Cumulative return\n",
    "    n = len(series)\n",
    "    if cum_return > 0 and n > 0:\n",
    "        return cum_return**(12/n) - 1\n",
    "    return series.mean() * 12 if n > 0 else np.nan\n",
    "\n",
    "# Define an aggregation function for performance metrics\n",
    "def agg_perf(df):\n",
    "    return pd.Series({\n",
    "        **{f'ann_excess_{h}m': annualize(df[f'excess_return_{h}m']) for h in ['1', '3', '12']},\n",
    "        **{f'hit_rate_{h}m': (df[f'excess_return_{h}m'] > 0).mean() for h in ['1', '3', '12']},\n",
    "        'n_obs': len(df)\n",
    "    })\n",
    "\n",
    "# Compute performance metrics for each period and net trade quintile\n",
    "performance_period = (\n",
    "    monthly_period\n",
    "    .groupby(['period', 'net_value_quintile'])\n",
    "    .apply(agg_perf, include_groups=False)\n",
    "    .reset_index()\n",
    "    .sort_values(['period', 'net_value_quintile'])\n",
    ")\n",
    "\n",
    "print(\"Performance by net trade quintile and period:\")\n",
    "print(performance_period.to_markdown(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run T‑Tests for Excess Returns by Period and Quintile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import ttest_1samp\n",
    "\n",
    "results_period = []\n",
    "# Group first by period, then by net trade quintile\n",
    "for period, period_df in valid.groupby('period'):\n",
    "    for quintile, df in period_df.groupby('net_value_quintile'):\n",
    "        for horizon in ['1', '3', '12']:\n",
    "            series = df[f'excess_return_{horizon}m'].dropna()\n",
    "            if len(series) < 5:  # Skip tiny samples\n",
    "                continue\n",
    "            t_stat, p_value = ttest_1samp(series, 0)\n",
    "            results_period.append({\n",
    "                'period': period,\n",
    "                'net_value_quintile': quintile,\n",
    "                'horizon_months': int(horizon),\n",
    "                'mean_excess_return': series.mean(),\n",
    "                't_stat': t_stat,\n",
    "                'p_value': p_value,\n",
    "                'n_obs': len(series)\n",
    "            })\n",
    "\n",
    "ttest_df_period = pd.DataFrame(results_period).sort_values(['period', 'net_value_quintile', 'horizon_months'])\n",
    "print(\"One-sample t-test of excess returns vs zero by period and quintile:\")\n",
    "print(ttest_df_period.to_markdown(index=False))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
