{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Non-derivative transaction data:\n",
      "| ACCESSION_NUMBER     | TRANS_DATE   | TRANS_CODE   |   EQUITY_SWAP_INVOLVED |   TRANS_SHARES |   TRANS_PRICEPERSHARE | TRANS_ACQUIRED_DISP_CD   |   SHRS_OWND_FOLWNG_TRANS | DIRECT_INDIRECT_OWNERSHIP   |   COMPANY_ID | RPTOWNERNAME     | RPTOWNER_RELATIONSHIP   | RPTOWNER_TITLE         |\n",
      "|:---------------------|:-------------|:-------------|-----------------------:|---------------:|----------------------:|:-------------------------|-------------------------:|:----------------------------|-------------:|:-----------------|:------------------------|:-----------------------|\n",
      "| 0000076605-17-000121 | 2017-09-27   | S            |                      0 |          10000 |                 83.16 | D                        |          15000           | D                           |       295249 | Cleveland Todd M | Director,Officer        | CEO                    |\n",
      "| 0001140361-17-037031 | 2017-09-27   | F            |                      0 |          28833 |                 16.46 | D                        |              3.36052e+06 | D                           |     54070696 | Olson Michael    | Officer                 | Chief Strategy Officer |\n",
      "| 0001140361-17-037030 | 2017-04-27   | M            |                      0 |          11875 |                  0    | A                        |              3.36052e+06 | D                           |     54070696 | Olson Michael    | Officer                 | Chief Strategy Officer |\n",
      "| 0001140361-17-037030 | 2017-09-15   | M            |                      0 |           9063 |                  0    | A                        |              3.36052e+06 | D                           |     54070696 | Olson Michael    | Officer                 | Chief Strategy Officer |\n",
      "| 0001140361-17-037030 | 2017-09-15   | M            |                      0 |           8333 |                  0    | A                        |              3.36052e+06 | D                           |     54070696 | Olson Michael    | Officer                 | Chief Strategy Officer |\n",
      "\n",
      "Returns and market cap data:\n",
      "| MONTH_END   |   COMPANY_ID |   RETURN_LEAD_1_MONTHS |   RETURN_LEAD_3_MONTHS |   RETURN_LEAD_12_MONTHS |   MARKET_CAP_USD |\n",
      "|:------------|-------------:|-----------------------:|-----------------------:|------------------------:|-----------------:|\n",
      "| 1994-07-31  |        18511 |             0.106531   |              0.109966  |                0.375126 |          2616.76 |\n",
      "| 1994-08-31  |        18511 |             0.0139749  |              0.043478  |                0.255416 |          2885.57 |\n",
      "| 1994-09-30  |        18511 |            -0.0107207  |              0.0316986 |                0.261565 |          3001.81 |\n",
      "| 1994-10-31  |        18511 |             0.0402486  |              0.083469  |                0.292618 |          3078.85 |\n",
      "| 1994-11-30  |        18511 |             0.00252853 |              0.0355451 |                0.279064 |          3069.56 |\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pandas.tseries.offsets import MonthEnd\n",
    "import time\n",
    "\n",
    "DATA_DIR = \"../data\"\n",
    "\n",
    "# returns_and_market_cap contains monthly returns and market cap data per company\n",
    "returns_and_market_cap = pd.read_csv(f\"{DATA_DIR}/returns_market_cap.csv\")\n",
    "\n",
    "# ndt contains non-derivative transaction data (e.g., insider trades)\n",
    "ndt = pd.read_csv(f\"{DATA_DIR}/ndt.csv\")\n",
    "\n",
    "print(\"Non-derivative transaction data:\")\n",
    "print(ndt.head().to_markdown(index=False))\n",
    "print()\n",
    "print(\"Returns and market cap data:\")\n",
    "print(returns_and_market_cap.head().to_markdown(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1: Data Preparation and Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Dropped 15 rows with invalid TRANS_DATE.\n",
      "Sample of trades data after preparation:\n",
      "| ACCESSION_NUMBER     | TRANS_DATE          | TRANS_CODE   |   EQUITY_SWAP_INVOLVED |   TRANS_SHARES |   TRANS_PRICEPERSHARE | TRANS_ACQUIRED_DISP_CD   |   SHRS_OWND_FOLWNG_TRANS | DIRECT_INDIRECT_OWNERSHIP   |   COMPANY_ID | RPTOWNERNAME   | RPTOWNER_RELATIONSHIP   | RPTOWNER_TITLE                 | month_end           |      trade_value |\n",
      "|:---------------------|:--------------------|:-------------|-----------------------:|---------------:|----------------------:|:-------------------------|-------------------------:|:----------------------------|-------------:|:---------------|:------------------------|:-------------------------------|:--------------------|-----------------:|\n",
      "| 0000001750-06-000002 | 2006-01-04 00:00:00 | S            |                      0 |         144360 |                 24.64 | D                        |                  6876.17 | D                           |       168154 | STORCH DAVID P | Director,Officer        | Chairman, Pres., CEO, Director | 2006-01-31 00:00:00 |      3.55703e+06 |\n",
      "| 0000001750-06-000054 | 2006-03-28 00:00:00 | S            |                      0 |          66200 |                 27.46 | D                        |                 18810    | D                           |       168154 | STORCH DAVID P | Director,Officer        | President, CEO & Director      | 2006-03-31 00:00:00 |      1.81785e+06 |\n",
      "| 0000001750-06-000056 | 2006-03-29 00:00:00 | S            |                      0 |         290912 |                 27.47 | D                        |                  6876.17 | D                           |       168154 | STORCH DAVID P | Director,Officer        | President, CEO & Director      | 2006-03-31 00:00:00 |      7.99135e+06 |\n",
      "| 0000001750-06-000060 | 2006-03-31 00:00:00 | S            |                      0 |          50000 |                 28.4  | D                        |                 18810    | D                           |       168154 | STORCH DAVID P | Director,Officer        | President, CEO & Director      | 2006-03-31 00:00:00 |      1.42e+06    |\n",
      "| 0000001750-06-000062 | 2006-04-03 00:00:00 | S            |                      0 |          24010 |                 29.14 | D                        |                 18810    | D                           |       168154 | STORCH DAVID P | Director,Officer        | President, CEO & Director      | 2006-04-30 00:00:00 | 699651           |\n",
      "\n",
      "Shape of trades data after preparation: (326634, 15)\n"
     ]
    }
   ],
   "source": [
    "# Convert date columns to datetime for consistent handling\n",
    "# MONTH_END in returns_and_market_cap is the end-of-month date for returns/market cap\n",
    "returns_and_market_cap['MONTH_END'] = pd.to_datetime(returns_and_market_cap['MONTH_END'])\n",
    "\n",
    "# TRANS_DATE in ndt is the transaction date; use errors='coerce' to handle invalid dates\n",
    "ndt['TRANS_DATE'] = pd.to_datetime(ndt['TRANS_DATE'], errors='coerce')\n",
    "\n",
    "# Create a month-end column in ndt by rounding transaction dates to the nearest month end\n",
    "ndt['month_end'] = ndt['TRANS_DATE'] + MonthEnd(0)\n",
    "\n",
    "# Check for and drop rows with invalid transaction dates (NaT)\n",
    "invalid_dates = ndt['TRANS_DATE'].isna().sum()\n",
    "if invalid_dates > 0:\n",
    "    print(f\"Warning: Dropped {invalid_dates} rows with invalid TRANS_DATE.\")\n",
    "    ndt = ndt.dropna(subset=['TRANS_DATE'])\n",
    "\n",
    "# Filter ndt to include only buy ('P') and sell ('S') transactions\n",
    "trades = ndt[ndt['TRANS_CODE'].isin(['P', 'S'])].copy()\n",
    "\n",
    "# Collapse duplicate rows so each ACCESSION_NUMBER is one trade\n",
    "trades = trades.groupby('ACCESSION_NUMBER', as_index=False).agg({\n",
    "    'TRANS_DATE': 'first',\n",
    "    'TRANS_CODE': 'first',\n",
    "    'EQUITY_SWAP_INVOLVED': 'first',\n",
    "    'TRANS_SHARES': 'sum',\n",
    "    'TRANS_PRICEPERSHARE': 'first',\n",
    "    'TRANS_ACQUIRED_DISP_CD': 'first',\n",
    "    'SHRS_OWND_FOLWNG_TRANS': 'first',\n",
    "    'DIRECT_INDIRECT_OWNERSHIP': 'first',\n",
    "    'COMPANY_ID': 'first',\n",
    "    'RPTOWNERNAME': 'first',\n",
    "    'RPTOWNER_RELATIONSHIP': 'first',\n",
    "    'RPTOWNER_TITLE': 'first',\n",
    "    'month_end': 'first'\n",
    "})\n",
    "\n",
    "\n",
    "# Calculate the dollar value of each trade (shares * price per share)\n",
    "trades['trade_value'] = trades['TRANS_SHARES'] * trades['TRANS_PRICEPERSHARE']\n",
    "\n",
    "print(\"Sample of trades data after preparation:\")\n",
    "print(trades.head().to_markdown(index=False))\n",
    "print()\n",
    "print(\"Shape of trades data after preparation:\", trades.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2: Market Returns Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample of market_return_1m market returns:\n",
      "| MONTH_END           |   market_return_1m |\n",
      "|:--------------------|-------------------:|\n",
      "| 1962-01-31 00:00:00 |          0.0886072 |\n",
      "| 1962-02-28 00:00:00 |         -0.0697672 |\n",
      "| 1962-03-31 00:00:00 |          0.0249994 |\n",
      "| 1962-04-30 00:00:00 |         -0.414634  |\n",
      "| 1962-05-31 00:00:00 |          0.0520832 |\n",
      "Sample of market_return_3m market returns:\n",
      "| MONTH_END           |   market_return_3m |\n",
      "|:--------------------|-------------------:|\n",
      "| 1962-01-31 00:00:00 |           0.037974 |\n",
      "| 1962-02-28 00:00:00 |          -0.44186  |\n",
      "| 1962-03-31 00:00:00 |          -0.36875  |\n",
      "| 1962-04-30 00:00:00 |          -0.256097 |\n",
      "| 1962-05-31 00:00:00 |           0.375    |\n",
      "Sample of market_return_12m market returns:\n",
      "| MONTH_END           |   market_return_12m |\n",
      "|:--------------------|--------------------:|\n",
      "| 1962-01-31 00:00:00 |           -0.319621 |\n",
      "| 1962-02-28 00:00:00 |           -0.331395 |\n",
      "| 1962-03-31 00:00:00 |           -0.234376 |\n",
      "| 1962-04-30 00:00:00 |           -0.14939  |\n",
      "| 1962-05-31 00:00:00 |            0.510417 |\n"
     ]
    }
   ],
   "source": [
    "# Compute total market capitalization per month for weighting returns\n",
    "# Sum MARKET_CAP_USD across all companies for each MONTH_END\n",
    "total_cap = returns_and_market_cap.groupby('MONTH_END')['MARKET_CAP_USD'].sum().rename('total_cap')\n",
    "\n",
    "# Join total_cap back to returns_and_market_cap for weight calculation\n",
    "returns_and_market_cap = returns_and_market_cap.join(total_cap, on='MONTH_END')\n",
    "\n",
    "# Calculate each company's weight as its market cap divided by total market cap\n",
    "returns_and_market_cap['weight'] = returns_and_market_cap['MARKET_CAP_USD'] / returns_and_market_cap['total_cap']\n",
    "\n",
    "# Initialize a dictionary to store market returns for different horizons (1, 3, 12 months)\n",
    "market_returns = {}\n",
    "\n",
    "# Compute weighted market returns for each horizon\n",
    "# For each month, multiply individual company returns by their weights and sum\n",
    "for horizon in ['1', '3', '12']:\n",
    "    market_returns[f'market_return_{horizon}m'] = returns_and_market_cap.groupby('MONTH_END').apply(\n",
    "        lambda df: np.sum(df[f'RETURN_LEAD_{horizon}_MONTHS'] * df['weight']),\n",
    "        include_groups=False  # Avoid including group keys in the apply function\n",
    "    ).reset_index(name=f'market_return_{horizon}m')\n",
    "\n",
    "for horizon in ['1', '3', '12']:\n",
    "    print(f\"Sample of market_return_{horizon}m market returns:\")\n",
    "    print(market_returns[f'market_return_{horizon}m'].head().to_markdown(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3: Merging Trades with Returns Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample of merged data with excess returns:\n",
      "|   COMPANY_ID | month_end           | TRANS_CODE   |   excess_return_1m |   excess_return_3m |   excess_return_12m |\n",
      "|-------------:|:--------------------|:-------------|-------------------:|-------------------:|--------------------:|\n",
      "|       168154 | 2006-01-31 00:00:00 | S            |          0.0584863 |          0.0745878 |           0.0870041 |\n",
      "|       168154 | 2006-03-31 00:00:00 | S            |         -0.0752978 |         -0.193422  |          -0.165043  |\n",
      "|       168154 | 2006-03-31 00:00:00 | S            |         -0.0752978 |         -0.193422  |          -0.165043  |\n",
      "|       168154 | 2006-03-31 00:00:00 | S            |         -0.0752978 |         -0.193422  |          -0.165043  |\n",
      "|       168154 | 2006-04-30 00:00:00 | S            |         -0.0544755 |         -0.0779137 |          -0.0112499 |\n"
     ]
    }
   ],
   "source": [
    "# Merge trades with returns_and_market_cap to align trade data with returns and market cap\n",
    "# Use left join to keep all trades, even if no matching returns data exists\n",
    "merged = trades.merge(returns_and_market_cap, \n",
    "                      left_on=['COMPANY_ID', 'month_end'], \n",
    "                      right_on=['COMPANY_ID', 'MONTH_END'], \n",
    "                      how='left').drop(columns='MONTH_END')  # Drop redundant MONTH_END column\n",
    "\n",
    "# Merge in the precomputed market returns for each horizon\n",
    "for horizon, df in market_returns.items():\n",
    "    merged = merged.merge(df, \n",
    "                          left_on='month_end', \n",
    "                          right_on='MONTH_END', \n",
    "                          how='left').drop(columns='MONTH_END')\n",
    "\n",
    "# Calculate excess returns for each horizon (company return - market return)\n",
    "for horizon in ['1', '3', '12']:\n",
    "    merged[f'excess_return_{horizon}m'] = merged[f'RETURN_LEAD_{horizon}_MONTHS'] - merged[f'market_return_{horizon}m']\n",
    "\n",
    "print(\"Sample of merged data with excess returns:\")\n",
    "print(merged[['COMPANY_ID', 'month_end', 'TRANS_CODE', 'excess_return_1m', 'excess_return_3m', 'excess_return_12m']].head().to_markdown(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4: Non-routine Transacitons and Quantile Assignemnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original shares ≤0: 0\n",
      "Pct sold >100%: 0\n",
      "Pct bought >100%: 8297\n",
      "\n",
      "Quantile distribution (sales):\n",
      "quantile_holdings_sale\n",
      "NaN    74340\n",
      "3.0    55622\n",
      "1.0    55622\n",
      "0.0    55622\n",
      "2.0    55621\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Quantile distribution (buys):\n",
      "quantile_holdings_buy\n",
      "NaN    222487\n",
      "0.0     18590\n",
      "3.0     18585\n",
      "2.0     18585\n",
      "1.0     18580\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Compute original (pre‑trade) shares\n",
    "merged['original_shares'] = np.where(\n",
    "    merged['TRANS_CODE'] == 'S',\n",
    "    merged['SHRS_OWND_FOLWNG_TRANS'] + merged['TRANS_SHARES'],\n",
    "    np.where(\n",
    "        merged['TRANS_CODE'] == 'P',\n",
    "        merged['SHRS_OWND_FOLWNG_TRANS'] - merged['TRANS_SHARES'],\n",
    "        np.nan\n",
    "    )\n",
    ")\n",
    "\n",
    "# Keep only trades where you actually held shares before the transaction\n",
    "merged = merged.query(\"original_shares > 0\").copy()\n",
    "\n",
    "# Calculate pct bought/sold safely\n",
    "merged['pct_holdings_sold'] = np.where(\n",
    "    merged['TRANS_CODE']=='S',\n",
    "    merged['TRANS_SHARES']/merged['original_shares'],\n",
    "    np.nan\n",
    ")\n",
    "\n",
    "merged['pct_holdings_bought'] = np.where(\n",
    "    merged['TRANS_CODE']=='P',\n",
    "    merged['TRANS_SHARES']/merged['original_shares'],\n",
    "    np.nan\n",
    ")\n",
    "\n",
    "# Function to assign quartile (or percentile if too few unique values)\n",
    "# def quantile_or_rank(series):\n",
    "#     nonnull = series.dropna()\n",
    "#     if nonnull.nunique() >= 4:\n",
    "#         return pd.qcut(series, 4, labels=False)\n",
    "#     return series.rank(pct=True)\n",
    "\n",
    "merged['quantile_holdings_sale'] = pd.qcut(merged['pct_holdings_sold'], 4, labels=False, duplicates='drop')\n",
    "merged['quantile_holdings_buy']  = pd.qcut(merged['pct_holdings_bought'], 4, labels=False, duplicates='drop')\n",
    "\n",
    "print(\"Original shares ≤0:\", (merged['original_shares'] <= 0).sum())\n",
    "print(\"Pct sold >100%:\", (merged['pct_holdings_sold'] > 1).sum())\n",
    "print(\"Pct bought >100%:\", (merged['pct_holdings_bought'] > 1).sum())\n",
    "print(\"\\nQuantile distribution (sales):\")\n",
    "print(merged['quantile_holdings_sale'].value_counts(dropna=False))\n",
    "print(\"\\nQuantile distribution (buys):\")\n",
    "print(merged['quantile_holdings_buy'].value_counts(dropna=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropped 3364 rows due to missing excess returns.\n",
      "\n",
      "Performance metrics for quantile_holdings_sale:\n",
      "|   quantile_holdings_sale |   avg_excess_1m |   ann_excess_1m |   count_1m |   hit_rate_1m |   avg_excess_3m |   ann_excess_3m |   count_3m |   hit_rate_3m |   avg_excess_12m |   ann_excess_12m |   count_12m |   hit_rate_12m |\n",
      "|-------------------------:|----------------:|----------------:|-----------:|--------------:|----------------:|----------------:|-----------:|--------------:|-----------------:|-----------------:|------------:|---------------:|\n",
      "|                        0 |     0.00227179  |      -0.0532656 |      55186 |      0.49585  |     0.00594013  |       -0.177564 |      55186 |      0.486138 |       0.0227138  |        -0.594463 |       54438 |       0.462164 |\n",
      "|                        1 |     0.00123608  |      -0.0525074 |      55162 |      0.493274 |     0.00115361  |       -0.181217 |      55162 |      0.482869 |       0.0153028  |        -0.531624 |       54532 |       0.470831 |\n",
      "|                        2 |     0.000840345 |      -0.0467182 |      55096 |      0.494501 |     0.00102981  |       -0.161576 |      55096 |      0.481868 |       0.0113992  |        -0.517476 |       54596 |       0.469762 |\n",
      "|                        3 |     0.0013699   |      -0.0431596 |      55083 |      0.500735 |     0.000585709 |       -0.162421 |      55083 |      0.480384 |       0.00910937 |        -0.494717 |       54632 |       0.468003 |\n",
      "\n",
      "Performance metrics for quantile_holdings_buy:\n",
      "|   quantile_holdings_buy |   avg_excess_1m |   ann_excess_1m |   count_1m |   hit_rate_1m |   avg_excess_3m |   ann_excess_3m |   count_3m |   hit_rate_3m |   avg_excess_12m |   ann_excess_12m |   count_12m |   hit_rate_12m |\n",
      "|------------------------:|----------------:|----------------:|-----------:|--------------:|----------------:|----------------:|-----------:|--------------:|-----------------:|-----------------:|------------:|---------------:|\n",
      "|                       0 |      0.0038498  |      -0.0450921 |      18223 |      0.491192 |      0.00773019 |       -0.149015 |      18223 |      0.455249 |        0.0178869 |        -0.680322 |       18143 |       0.362948 |\n",
      "|                       1 |      0.00565352 |      -0.032853  |      18224 |      0.500384 |      0.0103818  |       -0.159015 |      18224 |      0.470478 |        0.0138091 |        -0.667087 |       18075 |       0.413685 |\n",
      "|                       2 |      0.00734797 |      -0.0171303 |      18221 |      0.494649 |      0.0098524  |       -0.177491 |      18221 |      0.471489 |        0.0316757 |        -0.644592 |       18075 |       0.438121 |\n",
      "|                       3 |      0.0085926  |      -0.0100531 |      18268 |      0.490749 |      0.0133069  |       -0.174907 |      18268 |      0.475914 |        0.0440598 |        -0.637723 |       18101 |       0.455168 |\n",
      "\n",
      "Notes:\n",
      "- Annualized returns use geometric compounding when possible; otherwise, arithmetic mean * 12 is used.\n",
      "- Hit rate measures the proportion of observations with excess return > 0.\n",
      "- Counts show the number of observations for each return horizon.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/atang/Documents/verdad/insider-transaction-research/venv/lib/python3.13/site-packages/pandas/core/arraylike.py:399: RuntimeWarning: invalid value encountered in log1p\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "/Users/atang/Documents/verdad/insider-transaction-research/venv/lib/python3.13/site-packages/pandas/core/arraylike.py:399: RuntimeWarning: invalid value encountered in log1p\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "/Users/atang/Documents/verdad/insider-transaction-research/venv/lib/python3.13/site-packages/pandas/core/arraylike.py:399: RuntimeWarning: invalid value encountered in log1p\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "/Users/atang/Documents/verdad/insider-transaction-research/venv/lib/python3.13/site-packages/pandas/core/arraylike.py:399: RuntimeWarning: invalid value encountered in log1p\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
      "/Users/atang/Documents/verdad/insider-transaction-research/venv/lib/python3.13/site-packages/pandas/core/arraylike.py:399: RuntimeWarning: invalid value encountered in log1p\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "# ---------------------------\n",
    "# Performance Analysis for Individual Factors\n",
    "# ---------------------------\n",
    "# Filter rows with at least one non-null excess return (for 1, 3, or 12 months)\n",
    "excess_cols = [f'excess_return_{h}m' for h in ['1', '3', '12']]\n",
    "valid_nr = merged.dropna(subset=excess_cols, how='all').copy()\n",
    "print(f\"Dropped {len(merged) - len(valid_nr)} rows due to missing excess returns.\")\n",
    "\n",
    "def annualize(series):\n",
    "    series = series.dropna()\n",
    "    if series.empty:\n",
    "        return np.nan\n",
    "    n = len(series)\n",
    "    if n == 1:\n",
    "        return series.mean() * 12\n",
    "\n",
    "    log_cum = np.log1p(series).sum()\n",
    "    with np.errstate(over='ignore'):\n",
    "        result = np.exp(log_cum * 12/n) - 1\n",
    "    return series.mean() * 12 if np.isinf(result) else result\n",
    "\n",
    "# List both factor columns to evaluate (sales and buys)\n",
    "factors = ['quantile_holdings_sale', 'quantile_holdings_buy']\n",
    "horizons = ['1', '3', '12']\n",
    "\n",
    "for factor in factors:\n",
    "    grouped = valid_nr.groupby(factor)\n",
    "    metrics_df = pd.DataFrame({factor: grouped.size().index})\n",
    "    \n",
    "    for h in horizons:\n",
    "        col = f'excess_return_{h}m'\n",
    "        avg_excess = grouped[col].mean().rename(f'avg_excess_{h}m')\n",
    "        ann_excess = grouped[col].agg(annualize).rename(f'ann_excess_{h}m')\n",
    "        count_obs  = grouped[col].count().rename(f'count_{h}m')\n",
    "        hit_rate   = grouped[col].apply(lambda x: (x > 0).mean()).rename(f'hit_rate_{h}m')\n",
    "        metrics_df = pd.concat([metrics_df, avg_excess, ann_excess, count_obs, hit_rate], axis=1)\n",
    "    \n",
    "    print(f\"\\nPerformance metrics for {factor}:\")\n",
    "    print(metrics_df.reset_index(drop=True).to_markdown(index=False))\n",
    "\n",
    "print(\"\\nNotes:\")\n",
    "print(\"- Annualized returns use geometric compounding when possible; otherwise, arithmetic mean * 12 is used.\")\n",
    "print(\"- Hit rate measures the proportion of observations with excess return > 0.\")\n",
    "print(\"- Counts show the number of observations for each return horizon.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
